{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6070400,"sourceType":"datasetVersion","datasetId":3474395}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\nfrom datasets import load_dataset\n\nds = load_dataset(\"jxm/mpqa\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T07:46:10.650272Z","iopub.execute_input":"2026-02-14T07:46:10.651583Z","iopub.status.idle":"2026-02-14T07:46:29.028154Z","shell.execute_reply.started":"2026-02-14T07:46:10.651557Z","shell.execute_reply":"2026-02-14T07:46:29.027446Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e9f334a16d3489e840691cdc429b6fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-a7df005a1b0788(â€¦):   0%|          | 0.00/141k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e43cf2ecd64bed84c33f6634711e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001-05fc5ca1c399669(â€¦):   0%|          | 0.00/33.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7415354fe815462e978936048edcdb3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/dev-00000-of-00001-8814a3252cc44468(â€¦):   0%|          | 0.00/6.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67468cd5466c4fc8824babdb555bbccc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424ee8fb78874923aad9f9de20e897d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c26dba81b44c3d971e900b51c6942d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/256 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08bc07d6c6584bed93d37540391abe4a"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T07:46:29.029790Z","iopub.execute_input":"2026-02-14T07:46:29.030280Z","iopub.status.idle":"2026-02-14T07:46:29.035239Z","shell.execute_reply.started":"2026-02-14T07:46:29.030254Z","shell.execute_reply":"2026-02-14T07:46:29.034578Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label'],\n        num_rows: 8603\n    })\n    test: Dataset({\n        features: ['sentence', 'label'],\n        num_rows: 2000\n    })\n    dev: Dataset({\n        features: ['sentence', 'label'],\n        num_rows: 256\n    })\n})"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# **DistilBERT**","metadata":{}},{"cell_type":"code","source":"!pip install pennylane\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport torch\nfrom sklearn.metrics import (\n    roc_curve, roc_auc_score, classification_report, confusion_matrix\n)\nfrom sklearn.preprocessing import label_binarize\nimport pennylane as qml\nfrom pennylane import numpy as pnp\nimport re\nimport warnings\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nwarnings.filterwarnings('ignore')\n# Label encoding for 'status' column\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\nimport pennylane as qml\n# Convert data to PyTorch DataLoader format\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, roc_curve, auc\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import ParameterSampler\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport numpy as np\nimport torch\nfrom sklearn.preprocessing import label_binarize\nimport time\nimport random\nimport numpy as np\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModel\n\n# ---------------------------\n# Load MPQA Dataset\n# ---------------------------\nds = load_dataset(\"jxm/mpqa\")\n\n# Convert train split to pandas (MPQA already split)\ndf = ds[\"train\"].to_pandas()\n\nprint(df.head())\nprint(df.shape)\n\n# MPQA typically has columns: 'text' and 'label'\n# If column name is different, print df.columns to check.\n\n# Rename for compatibility with your previous code\ndf = df.rename(columns={\"text\": \"sentence\"})\n\n# ---------------------------\n# Basic Preprocessing\n# ---------------------------\n\ndef preprocess_text(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    text = re.sub(r'@\\w+', '', text)\n    text = re.sub(r'#\\w+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = text.lower()\n    return text\n\ndf['cleaned_text'] = df['sentence'].apply(preprocess_text)\n\n# ---------------------------\n# Remove Stopwords\n# ---------------------------\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef clean_statement(statement):\n    statement = statement.lower()\n    statement = re.sub(r'[^\\w\\s]', '', statement)\n    statement = re.sub(r'\\d+', '', statement)\n    words = statement.split()\n    words = [word for word in words if word not in stop_words]\n    cleaned_statement = ' '.join(words)\n    return cleaned_statement\n\ndf['cleaned_text'] = df['cleaned_text'].apply(clean_statement)\n\nprint(df.head())\n\n# ---------------------------\n# Label Encoding\n# ---------------------------\n# MPQA labels are usually already numeric (0,1)\n# But we keep this step for safety\n\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['label'])\n\nprint(df['label'].value_counts())\n\n# ---------------------------\n# Balance Dataset\n# ---------------------------\n\nmajority_class = df['label'].value_counts().idxmax()\nminority_class = df['label'].value_counts().idxmin()\n\ndf_majority = df[df['label'] == majority_class]\ndf_minority = df[df['label'] == minority_class]\n\n# Downsample majority\ndf_majority_downsampled = df_majority.sample(len(df_minority), random_state=42)\n\n# Combine\ndf_balanced = pd.concat([df_majority_downsampled, df_minority])\n\n# Shuffle\ndf_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"Balanced Class Distribution:\")\nprint(df_balanced['label'].value_counts())\n\ndf_balanced.head()\n\n\n# Initialize the tokenizer and model from Hugging Face\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ntokens = tokenizer.batch_encode_plus(\n    df['cleaned_text'].tolist(),\n    max_length=128,\n    padding=\"max_length\",   # <-- updated\n    truncation=True,\n    return_tensors=\"pt\"\n)\n\n# Convert to tensors\ninput_ids = torch.tensor(tokens['input_ids'])\nattention_masks = torch.tensor(tokens['attention_mask'])\nlabels = torch.tensor(df['label'].values)\n\n# Split the data into training and validation sets\ntrain_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.2)\ntrain_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.2)\n\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\n\ntrain_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=32)\nval_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=32)\n\n# Create a quantum node (circuit)\ndev = qml.device(\"default.qubit\", wires=2)  # or 'default.qubit'\n\n@qml.qnode(dev)\n@qml.qnode(dev)\ndef quantum_circuit(weights, inputs):\n    qml.Hadamard(wires=0)\n    qml.Hadamard(wires=1)\n\n    qml.RX(inputs[0], wires=0)\n    qml.RY(inputs[1], wires=1)\n\n    qml.CNOT(wires=[0, 1])\n\n    qml.RZ(weights[0], wires=0)\n    qml.RZ(weights[1], wires=1)\n\n    return qml.expval(qml.PauliZ(0))\n\nclass QBiLSTM(nn.Module):\n    def __init__(self):\n        super(QBiLSTM, self).__init__()\n\n        self.bert = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n\n        # ðŸ” BiLSTM instead of LSTM\n        self.bilstm = nn.LSTM(\n            input_size=768,\n            hidden_size=128,\n            batch_first=True,\n            bidirectional=True\n        )\n\n        # 128 Ã— 2 because BiLSTM\n        self.fc = nn.Linear(256, 2)\n\n    def quantum_layer(self, inputs):\n        processed_features = []\n\n        for feature_vector in inputs:\n            features_for_quantum = feature_vector[:2]\n            q_out = quantum_circuit(\n                torch.randn(2, dtype=torch.float32),\n                features_for_quantum\n            )\n            processed_features.append(q_out)\n\n        return torch.stack(processed_features).unsqueeze(1)\n\n    def forward(self, input_ids, attention_mask):\n        bert_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # BiLSTM output\n        lstm_output, _ = self.bilstm(bert_output.last_hidden_state)\n\n        # Last timestep (batch, 256)\n        last_hidden = lstm_output[:, -1, :]\n\n        _ = self.quantum_layer(last_hidden)\n\n        # Classification\n        output = self.fc(last_hidden)\n        return output\n\n\nmodel = QBiLSTM()\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define hyperparameter search space\nparam_grid = {\n    'learning_rate': [1e-5, 2e-5, 3e-5, 5e-5],\n    'batch_size': [16, 32],\n    'epochs': [3, 5, 7]\n}\n\nnum_samples = 5  \nparam_list = list(ParameterSampler(param_grid, n_iter=num_samples, random_state=42))\n\nbest_model = None\nbest_val_accuracy = 0.0\nbest_params = None\n\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Loop through each random set of hyperparameters\nfor idx, params in enumerate(param_list):\n    print(f\"Testing configuration {idx + 1}: {params}\")\n\n    learning_rate = params['learning_rate']\n    batch_size = params['batch_size']\n    epochs = params['epochs']\n\n    # Define optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n    for epoch in range(epochs):\n        start_time = time.time()\n        model.train()\n        total_train_loss = 0\n        total_train_accuracy = 0\n\n        for batch in train_dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n\n            # Move tensors to the same device\n            b_input_ids = b_input_ids.to(device)\n            b_input_mask = b_input_mask.to(device)\n            b_labels = b_labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(b_input_ids, b_input_mask)\n            loss = nn.CrossEntropyLoss()(outputs, b_labels)\n            total_train_loss += loss.item()\n\n            logits = outputs.detach().cpu().numpy()\n            label_ids = b_labels.cpu().numpy()\n            total_train_accuracy += flat_accuracy(logits, label_ids)\n\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n        avg_train_loss = total_train_loss / len(train_dataloader)\n        avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n\n        # Validation\n        model.eval()\n        total_val_loss = 0\n        total_val_accuracy = 0\n        all_preds = []\n        all_labels = []\n\n        for batch in val_dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n\n            # Move tensors to the same device\n            b_input_ids = b_input_ids.to(device)\n            b_input_mask = b_input_mask.to(device)\n            b_labels = b_labels.to(device)\n\n            with torch.no_grad():\n                outputs = model(b_input_ids, b_input_mask)\n                loss = nn.CrossEntropyLoss()(outputs, b_labels)\n                total_val_loss += loss.item()\n\n                logits = outputs.detach().cpu().numpy()\n                label_ids = b_labels.cpu().numpy()\n                total_val_accuracy += flat_accuracy(logits, label_ids)\n\n                all_preds.extend(np.argmax(logits, axis=1).flatten())\n                all_labels.extend(label_ids.flatten())\n\n        avg_val_loss = total_val_loss / len(val_dataloader)\n        avg_val_accuracy = total_val_accuracy / len(val_dataloader)\n        elapsed_time = time.time() - start_time\n\n        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Train Accuracy: {avg_train_accuracy:.4f}\")\n        print(f\"Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {avg_val_accuracy:.4f}\")\n        print(f\"Time: {elapsed_time:.2f} seconds\")\n\n        # Save best model\n        if avg_val_accuracy > best_val_accuracy:\n            best_val_accuracy = avg_val_accuracy\n            best_model = model.state_dict()\n            best_params = params\n            torch.save(best_model, 'best_model.pth')\n            print(f\"New best model saved with accuracy: {best_val_accuracy:.4f}\")\n\n# Generate classification report\nprint(\"Classification Report:\")\nprint(classification_report(all_labels, all_preds))\n\n\n\n# Define search space for hyperparameters\nepochs = 5\nlearning_rates = [2e-5, 3e-5, 5e-5]\nbatch_sizes = [16, 32]\noptimizers = ['adamw', 'adam', 'rmsprop', 'sgd']\n\n# Number of random samples to try\nnum_samples = 5\n\n# Function to calculate accuracy\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Function to get optimizer\ndef get_optimizer(optimizer_name, model_parameters, lr):\n    if optimizer_name == 'adamw':\n        return optim.AdamW(model_parameters, lr=lr)\n    elif optimizer_name == 'adam':\n        return optim.Adam(model_parameters, lr=lr)\n    elif optimizer_name == 'rmsprop':\n        return optim.RMSprop(model_parameters, lr=lr)\n    elif optimizer_name == 'sgd':\n        return optim.SGD(model_parameters, lr=lr)\n\n# Randomly sample hyperparameter combinations\nrandom_hyperparams = [\n    {\n        \"optimizer\": random.choice(optimizers),\n        \"learning_rate\": random.choice(learning_rates),\n        \"batch_size\": random.choice(batch_sizes),\n    }\n    for _ in range(num_samples)\n]\n\nbest_accuracy = 0\nbest_params = {}\n\n# Iterate over randomly chosen hyperparameter sets\nfor params in random_hyperparams:\n    optimizer_name = params[\"optimizer\"]\n    lr = params[\"learning_rate\"]\n    batch_size = params[\"batch_size\"]\n\n    # Initialize data loaders\n    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n    val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n\n    # Define optimizer and scheduler\n    optimizer = get_optimizer(optimizer_name, model.parameters(), lr)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n    # Move model to device\n    model.to(device)\n\n    for epoch in range(epochs):\n        start_time = time.time()\n\n        # Training\n        model.train()\n        total_train_loss = 0\n        total_train_accuracy = 0\n        for batch in train_dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(b_input_ids, b_input_mask)\n            loss = nn.CrossEntropyLoss()(outputs, b_labels)\n            total_train_loss += loss.item()\n\n            logits = outputs.detach().cpu().numpy()\n            label_ids = b_labels.cpu().numpy()\n            total_train_accuracy += flat_accuracy(logits, label_ids)\n\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n        avg_train_loss = total_train_loss / len(train_dataloader)\n        avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n\n        # Validation\n        model.eval()\n        total_val_loss = 0\n        total_val_accuracy = 0\n        for batch in val_dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n\n            with torch.no_grad():\n                outputs = model(b_input_ids, b_input_mask)\n                loss = nn.CrossEntropyLoss()(outputs, b_labels)\n                total_val_loss += loss.item()\n\n                logits = outputs.detach().cpu().numpy()\n                label_ids = b_labels.cpu().numpy()\n                total_val_accuracy += flat_accuracy(logits, label_ids)\n\n        avg_val_loss = total_val_loss / len(val_dataloader)\n        avg_val_accuracy = total_val_accuracy / len(val_dataloader)\n\n        elapsed_time = time.time() - start_time\n\n        print(f\"Optimizer: {optimizer_name} | Learning Rate: {lr} | Batch Size: {batch_size}\")\n        print(f\"Epoch {epoch+1}\")\n        print(f\"Train Loss: {avg_train_loss:.4f} | Train Accuracy: {avg_train_accuracy:.4f}\")\n        print(f\"Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {avg_val_accuracy:.4f}\")\n        print(f\"Time: {elapsed_time:.2f} seconds\")\n        print(\"-\" * 50)\n\n        # Update best parameters if validation accuracy improves\n        if avg_val_accuracy > best_accuracy:\n            best_accuracy = avg_val_accuracy\n            best_params = {\"optimizer\": optimizer_name, \"learning_rate\": lr, \"batch_size\": batch_size}\n\nprint(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\nprint(f\"Best Parameters: Optimizer = {best_params['optimizer']}, Learning Rate = {best_params['learning_rate']}, Batch Size = {best_params['batch_size']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T04:48:11.851803Z","iopub.execute_input":"2026-02-12T04:48:11.852162Z","iopub.status.idle":"2026-02-12T05:52:54.754224Z","shell.execute_reply.started":"2026-02-12T04:48:11.852129Z","shell.execute_reply":"2026-02-12T05:52:54.753559Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pennylane in /usr/local/lib/python3.12/dist-packages (0.44.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.15.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\nRequirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.17.1)\nRequirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\nRequirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.4.4)\nRequirement already satisfied: autoray==0.8.2 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.8.2)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\nRequirement already satisfied: pennylane-lightning>=0.44 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.44.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.5)\nRequirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (26.0rc2)\nRequirement already satisfied: diastatic-malt in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.15.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\nRequirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning>=0.44->pennylane) (0.3.31.22.1)\nRequirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\nRequirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2026.1.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\nRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n                                  sentence  label\n0         would not find it at all strange      0\n1  that four shots would solve the problem      0\n2                                  in turn      0\n3                                  because      0\n4                                 regained      0\n(8603, 2)\n                                  sentence  label  \\\n0         would not find it at all strange      0   \n1  that four shots would solve the problem      0   \n2                                  in turn      0   \n3                                  because      0   \n4                                 regained      0   \n\n                     cleaned_text  \n0              would find strange  \n1  four shots would solve problem  \n2                            turn  \n3                                  \n4                        regained  \nlabel\n0    6292\n1    2311\nName: count, dtype: int64\nBalanced Class Distribution:\nlabel\n0    2311\n1    2311\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5230e6f0d2d04c98861877191a532fac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"009c22481a5d4b5d9ebc523d4a813cdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cd43503ae0c4f9484a7a4e2d8d743b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a817be9b8542e69f0509229dfe2f5b"}},"metadata":{}},{"name":"stderr","text":"2026-02-12 04:48:20.914159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770871701.084680      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770871701.134611      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770871701.543849      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770871701.543904      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770871701.543908      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770871701.543910      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d3ee259ca0440d960c44b7a8f3c17a"}},"metadata":{}},{"name":"stdout","text":"Testing configuration 1: {'learning_rate': 1e-05, 'epochs': 7, 'batch_size': 16}\nEpoch 1 | Train Loss: 0.4572 | Train Accuracy: 0.7857\nValidation Loss: 0.3413 | Validation Accuracy: 0.8560\nTime: 74.61 seconds\nNew best model saved with accuracy: 0.8560\nEpoch 2 | Train Loss: 0.3040 | Train Accuracy: 0.8733\nValidation Loss: 0.3301 | Validation Accuracy: 0.8648\nTime: 73.44 seconds\nNew best model saved with accuracy: 0.8648\nEpoch 3 | Train Loss: 0.2636 | Train Accuracy: 0.8954\nValidation Loss: 0.3282 | Validation Accuracy: 0.8729\nTime: 73.20 seconds\nNew best model saved with accuracy: 0.8729\nEpoch 4 | Train Loss: 0.2320 | Train Accuracy: 0.9126\nValidation Loss: 0.3398 | Validation Accuracy: 0.8729\nTime: 74.00 seconds\nEpoch 5 | Train Loss: 0.2112 | Train Accuracy: 0.9198\nValidation Loss: 0.3385 | Validation Accuracy: 0.8754\nTime: 74.54 seconds\nNew best model saved with accuracy: 0.8754\nEpoch 6 | Train Loss: 0.1890 | Train Accuracy: 0.9314\nValidation Loss: 0.3470 | Validation Accuracy: 0.8735\nTime: 74.53 seconds\nEpoch 7 | Train Loss: 0.1794 | Train Accuracy: 0.9368\nValidation Loss: 0.3509 | Validation Accuracy: 0.8729\nTime: 74.73 seconds\nTesting configuration 2: {'learning_rate': 1e-05, 'epochs': 5, 'batch_size': 32}\nEpoch 1 | Train Loss: 0.1944 | Train Accuracy: 0.9288\nValidation Loss: 0.3612 | Validation Accuracy: 0.8705\nTime: 75.22 seconds\nEpoch 2 | Train Loss: 0.1636 | Train Accuracy: 0.9407\nValidation Loss: 0.3773 | Validation Accuracy: 0.8706\nTime: 74.39 seconds\nEpoch 3 | Train Loss: 0.1419 | Train Accuracy: 0.9488\nValidation Loss: 0.3809 | Validation Accuracy: 0.8751\nTime: 75.35 seconds\nEpoch 4 | Train Loss: 0.1317 | Train Accuracy: 0.9510\nValidation Loss: 0.3941 | Validation Accuracy: 0.8681\nTime: 74.71 seconds\nEpoch 5 | Train Loss: 0.1222 | Train Accuracy: 0.9572\nValidation Loss: 0.3928 | Validation Accuracy: 0.8774\nTime: 74.76 seconds\nNew best model saved with accuracy: 0.8774\nTesting configuration 3: {'learning_rate': 1e-05, 'epochs': 3, 'batch_size': 16}\nEpoch 1 | Train Loss: 0.1324 | Train Accuracy: 0.9502\nValidation Loss: 0.4115 | Validation Accuracy: 0.8705\nTime: 73.93 seconds\nEpoch 2 | Train Loss: 0.1220 | Train Accuracy: 0.9562\nValidation Loss: 0.4332 | Validation Accuracy: 0.8686\nTime: 74.01 seconds\nEpoch 3 | Train Loss: 0.1028 | Train Accuracy: 0.9624\nValidation Loss: 0.4238 | Validation Accuracy: 0.8693\nTime: 74.29 seconds\nTesting configuration 4: {'learning_rate': 3e-05, 'epochs': 5, 'batch_size': 32}\nEpoch 1 | Train Loss: 0.1465 | Train Accuracy: 0.9443\nValidation Loss: 0.4789 | Validation Accuracy: 0.8503\nTime: 74.18 seconds\nEpoch 2 | Train Loss: 0.1215 | Train Accuracy: 0.9536\nValidation Loss: 0.4361 | Validation Accuracy: 0.8614\nTime: 74.14 seconds\nEpoch 3 | Train Loss: 0.0929 | Train Accuracy: 0.9630\nValidation Loss: 0.4911 | Validation Accuracy: 0.8572\nTime: 74.23 seconds\nEpoch 4 | Train Loss: 0.0786 | Train Accuracy: 0.9688\nValidation Loss: 0.4876 | Validation Accuracy: 0.8660\nTime: 74.26 seconds\nEpoch 5 | Train Loss: 0.0679 | Train Accuracy: 0.9722\nValidation Loss: 0.5218 | Validation Accuracy: 0.8625\nTime: 74.09 seconds\nTesting configuration 5: {'learning_rate': 5e-05, 'epochs': 7, 'batch_size': 16}\nEpoch 1 | Train Loss: 0.1233 | Train Accuracy: 0.9510\nValidation Loss: 0.4756 | Validation Accuracy: 0.8567\nTime: 73.95 seconds\nEpoch 2 | Train Loss: 0.1021 | Train Accuracy: 0.9573\nValidation Loss: 0.4935 | Validation Accuracy: 0.8610\nTime: 74.26 seconds\nEpoch 3 | Train Loss: 0.0767 | Train Accuracy: 0.9667\nValidation Loss: 0.4715 | Validation Accuracy: 0.8631\nTime: 73.19 seconds\nEpoch 4 | Train Loss: 0.0685 | Train Accuracy: 0.9670\nValidation Loss: 0.5589 | Validation Accuracy: 0.8654\nTime: 73.34 seconds\nEpoch 5 | Train Loss: 0.0545 | Train Accuracy: 0.9744\nValidation Loss: 0.5754 | Validation Accuracy: 0.8660\nTime: 73.36 seconds\nEpoch 6 | Train Loss: 0.0520 | Train Accuracy: 0.9757\nValidation Loss: 0.6345 | Validation Accuracy: 0.8612\nTime: 73.33 seconds\nEpoch 7 | Train Loss: 0.0451 | Train Accuracy: 0.9774\nValidation Loss: 0.6317 | Validation Accuracy: 0.8706\nTime: 73.28 seconds\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.91      0.91      1263\n           1       0.76      0.75      0.76       458\n\n    accuracy                           0.87      1721\n   macro avg       0.83      0.83      0.83      1721\nweighted avg       0.87      0.87      0.87      1721\n\nOptimizer: adam | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 1\nTrain Loss: 0.0737 | Train Accuracy: 0.9671\nValidation Loss: 0.5704 | Validation Accuracy: 0.8625\nTime: 76.72 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 2\nTrain Loss: 0.0591 | Train Accuracy: 0.9730\nValidation Loss: 0.6336 | Validation Accuracy: 0.8469\nTime: 76.76 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 3\nTrain Loss: 0.0517 | Train Accuracy: 0.9772\nValidation Loss: 0.6399 | Validation Accuracy: 0.8619\nTime: 76.76 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 4\nTrain Loss: 0.0428 | Train Accuracy: 0.9785\nValidation Loss: 0.6892 | Validation Accuracy: 0.8625\nTime: 77.29 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 5\nTrain Loss: 0.0400 | Train Accuracy: 0.9791\nValidation Loss: 0.7084 | Validation Accuracy: 0.8660\nTime: 76.75 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 1\nTrain Loss: 0.0560 | Train Accuracy: 0.9731\nValidation Loss: 0.6842 | Validation Accuracy: 0.8637\nTime: 73.88 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 2\nTrain Loss: 0.0448 | Train Accuracy: 0.9773\nValidation Loss: 0.7729 | Validation Accuracy: 0.8498\nTime: 73.18 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 3\nTrain Loss: 0.0395 | Train Accuracy: 0.9802\nValidation Loss: 0.7306 | Validation Accuracy: 0.8666\nTime: 72.54 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 4\nTrain Loss: 0.0374 | Train Accuracy: 0.9796\nValidation Loss: 0.7746 | Validation Accuracy: 0.8619\nTime: 72.59 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 5\nTrain Loss: 0.0361 | Train Accuracy: 0.9802\nValidation Loss: 0.7719 | Validation Accuracy: 0.8660\nTime: 72.67 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 5e-05 | Batch Size: 32\nEpoch 1\nTrain Loss: 0.0607 | Train Accuracy: 0.9729\nValidation Loss: 0.6315 | Validation Accuracy: 0.8614\nTime: 73.13 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 5e-05 | Batch Size: 32\nEpoch 2\nTrain Loss: 0.0572 | Train Accuracy: 0.9750\nValidation Loss: 0.6196 | Validation Accuracy: 0.8550\nTime: 73.06 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 5e-05 | Batch Size: 32\nEpoch 3\nTrain Loss: 0.0452 | Train Accuracy: 0.9776\nValidation Loss: 0.7358 | Validation Accuracy: 0.8544\nTime: 73.24 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 5e-05 | Batch Size: 32\nEpoch 4\nTrain Loss: 0.0393 | Train Accuracy: 0.9789\nValidation Loss: 0.7679 | Validation Accuracy: 0.8573\nTime: 73.17 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 5e-05 | Batch Size: 32\nEpoch 5\nTrain Loss: 0.0362 | Train Accuracy: 0.9806\nValidation Loss: 0.7776 | Validation Accuracy: 0.8591\nTime: 73.14 seconds\n--------------------------------------------------\nOptimizer: adamw | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 1\nTrain Loss: 0.0497 | Train Accuracy: 0.9756\nValidation Loss: 0.7737 | Validation Accuracy: 0.8504\nTime: 77.57 seconds\n--------------------------------------------------\nOptimizer: adamw | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 2\nTrain Loss: 0.0409 | Train Accuracy: 0.9781\nValidation Loss: 0.8044 | Validation Accuracy: 0.8550\nTime: 78.20 seconds\n--------------------------------------------------\nOptimizer: adamw | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 3\nTrain Loss: 0.0398 | Train Accuracy: 0.9793\nValidation Loss: 0.8195 | Validation Accuracy: 0.8538\nTime: 77.22 seconds\n--------------------------------------------------\nOptimizer: adamw | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 4\nTrain Loss: 0.0376 | Train Accuracy: 0.9801\nValidation Loss: 0.8314 | Validation Accuracy: 0.8533\nTime: 77.20 seconds\n--------------------------------------------------\nOptimizer: adamw | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 5\nTrain Loss: 0.0374 | Train Accuracy: 0.9810\nValidation Loss: 0.8512 | Validation Accuracy: 0.8550\nTime: 77.23 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 1\nTrain Loss: 0.0344 | Train Accuracy: 0.9816\nValidation Loss: 0.8480 | Validation Accuracy: 0.8556\nTime: 71.20 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 2\nTrain Loss: 0.0343 | Train Accuracy: 0.9816\nValidation Loss: 0.8481 | Validation Accuracy: 0.8556\nTime: 71.12 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 3\nTrain Loss: 0.0352 | Train Accuracy: 0.9816\nValidation Loss: 0.8481 | Validation Accuracy: 0.8556\nTime: 71.12 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 4\nTrain Loss: 0.0351 | Train Accuracy: 0.9816\nValidation Loss: 0.8481 | Validation Accuracy: 0.8556\nTime: 71.32 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 5\nTrain Loss: 0.0344 | Train Accuracy: 0.9818\nValidation Loss: 0.8481 | Validation Accuracy: 0.8556\nTime: 71.15 seconds\n--------------------------------------------------\nBest Validation Accuracy: 0.8666\nBest Parameters: Optimizer = rmsprop, Learning Rate = 2e-05, Batch Size = 32\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score,\n    f1_score, roc_auc_score, confusion_matrix,\n    classification_report, roc_curve\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\ndef evaluate_binary_model(model, dataloader, device):\n    model.eval()\n    all_true_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids = b_input_ids.to(device)\n            b_input_mask = b_input_mask.to(device)\n            b_labels = b_labels.to(device)\n\n            outputs = model(b_input_ids, b_input_mask)\n            probs = torch.softmax(outputs, dim=1)\n\n            all_true_labels.append(b_labels.cpu().numpy())\n            all_probs.append(probs.cpu().numpy())\n\n    # Convert to numpy\n    all_true_labels = np.concatenate(all_true_labels)\n    all_probs = np.concatenate(all_probs)\n    all_preds = np.argmax(all_probs, axis=1)\n\n    # ================= Metrics =================\n    tn, fp, fn, tp = confusion_matrix(all_true_labels, all_preds).ravel()\n\n    accuracy = accuracy_score(all_true_labels, all_preds)\n    precision = precision_score(all_true_labels, all_preds)\n    recall = recall_score(all_true_labels, all_preds)          # Sensitivity\n    specificity = tn / (tn + fp)\n    f1 = f1_score(all_true_labels, all_preds)\n    auc = roc_auc_score(all_true_labels, all_probs[:, 1])\n\n    # ================= Print metrics =================\n    print(\"===== Evaluation Metrics =====\")\n    print(f\"Accuracy     : {accuracy:.4f}\")\n    print(f\"Precision    : {precision:.4f}\")\n    print(f\"Recall       : {recall:.4f}\")\n    print(f\"Sensitivity : {recall:.4f}\")\n    print(f\"Specificity : {specificity:.4f}\")\n    print(f\"F1-score    : {f1:.4f}\")\n    print(f\"AUC         : {auc:.4f}\")\n\n    # ================= Classification Report =================\n    print(\"\\n===== Classification Report =====\")\n    print(classification_report(all_true_labels, all_preds, digits=4))\n\n    # ================= ROC Curve (Both Classes) =================\n    fpr_0, tpr_0, _ = roc_curve(all_true_labels, all_probs[:, 0], pos_label=0)\n    fpr_1, tpr_1, _ = roc_curve(all_true_labels, all_probs[:, 1], pos_label=1)\n\n    auc_0 = roc_auc_score(1 - all_true_labels, all_probs[:, 0])\n    auc_1 = roc_auc_score(all_true_labels, all_probs[:, 1])\n\n    plt.figure(figsize=(7, 6))\n    plt.plot(fpr_0, tpr_0, label=f\"Class 0 ROC (AUC = {auc_0:.2f})\", lw=2)\n    plt.plot(fpr_1, tpr_1, label=f\"Class 1 ROC (AUC = {auc_1:.2f})\", lw=2)\n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curve for Binary Classification\")\n    plt.legend(loc=\"lower right\")\n    plt.grid()\n    plt.tight_layout()\n    plt.show()\n\n    return accuracy, precision, recall, specificity, f1, auc\n\n\naccuracy, precision, recall, specificity, f1, auc = evaluate_binary_model(\n    model, val_dataloader, device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T05:52:54.755678Z","iopub.execute_input":"2026-02-12T05:52:54.756342Z","iopub.status.idle":"2026-02-12T05:53:03.602576Z","shell.execute_reply.started":"2026-02-12T05:52:54.756314Z","shell.execute_reply":"2026-02-12T05:53:03.601976Z"}},"outputs":[{"name":"stdout","text":"===== Evaluation Metrics =====\nAccuracy     : 0.8553\nPrecision    : 0.7238\nRecall       : 0.7380\nSensitivity : 0.7380\nSpecificity : 0.8979\nF1-score    : 0.7308\nAUC         : 0.8906\n\n===== Classification Report =====\n              precision    recall  f1-score   support\n\n           0     0.9043    0.8979    0.9011      1263\n           1     0.7238    0.7380    0.7308       458\n\n    accuracy                         0.8553      1721\n   macro avg     0.8140    0.8179    0.8159      1721\nweighted avg     0.8563    0.8553    0.8558      1721\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 700x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import re, time, random, warnings\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pennylane as qml\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, accuracy_score\n\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.optim import AdamW\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nwarnings.filterwarnings(\"ignore\")\nnltk.download(\"stopwords\")\nstop_words = set(stopwords.words(\"english\"))\n\n\n# ---------------------------\n# Load MPQA Dataset\n# ---------------------------\nds = load_dataset(\"jxm/mpqa\")\n\n# Convert train split to pandas (MPQA already split)\ndf = ds[\"train\"].to_pandas()\n\nprint(df.head())\nprint(df.shape)\n\n# MPQA typically has columns: 'text' and 'label'\n# If column name is different, print df.columns to check.\n\n# Rename for compatibility with your previous code\ndf = df.rename(columns={\"text\": \"sentence\"})\n\n# ---------------------------\n# Basic Preprocessing\n# ---------------------------\n\ndef preprocess_text(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    text = re.sub(r'@\\w+', '', text)\n    text = re.sub(r'#\\w+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = text.lower()\n    return text\n\ndf['cleaned_text'] = df['sentence'].apply(preprocess_text)\n\n# ---------------------------\n# Remove Stopwords\n# ---------------------------\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef clean_statement(statement):\n    statement = statement.lower()\n    statement = re.sub(r'[^\\w\\s]', '', statement)\n    statement = re.sub(r'\\d+', '', statement)\n    words = statement.split()\n    words = [word for word in words if word not in stop_words]\n    cleaned_statement = ' '.join(words)\n    return cleaned_statement\n\ndf['cleaned_text'] = df['cleaned_text'].apply(clean_statement)\n\nprint(df.head())\n\n# ---------------------------\n# Label Encoding\n# ---------------------------\n# MPQA labels are usually already numeric (0,1)\n# But we keep this step for safety\n\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['label'])\n\nprint(df['label'].value_counts())\n\n# ---------------------------\n# Balance Dataset\n# ---------------------------\n\nmajority_class = df['label'].value_counts().idxmax()\nminority_class = df['label'].value_counts().idxmin()\n\ndf_majority = df[df['label'] == majority_class]\ndf_minority = df[df['label'] == minority_class]\n\n# Downsample majority\ndf_majority_downsampled = df_majority.sample(len(df_minority), random_state=42)\n\n# Combine\ndf_balanced = pd.concat([df_majority_downsampled, df_minority])\n\n# Shuffle\ndf_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"Balanced Class Distribution:\")\nprint(df_balanced['label'].value_counts())\n\ndf_balanced.head()\n\nprint(df_balanced[\"label\"].value_counts())\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ntokens = tokenizer(\n    df_balanced[\"cleaned_text\"].tolist(),\n    padding=\"max_length\",\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)\n\ninput_ids = tokens[\"input_ids\"]\nattention_mask = tokens[\"attention_mask\"]\nlabels = torch.tensor(df_balanced[\"label\"].values)\nX_train, X_val, y_train, y_val, m_train, m_val = train_test_split(\n    input_ids, labels, attention_mask, test_size=0.2, random_state=42\n)\n\ntrain_data = TensorDataset(X_train, m_train, y_train)\nval_data = TensorDataset(X_val, m_val, y_val)\n\ntrain_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=16)\nval_loader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=16)\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc1(weights, x):\n    qml.Hadamard(0)\n    qml.Hadamard(1)\n    qml.RX(x[0], 0)\n    qml.RY(x[1], 1)\n    qml.CNOT([0,1])\n    qml.RZ(weights[0], 0)\n    qml.RZ(weights[1], 1)\n    return qml.expval(qml.PauliZ(0))\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc2(weights, x):\n    qml.Hadamard(0)\n    qml.RY(x[0], 0)\n    qml.RX(x[1], 1)\n    qml.CNOT([1,0])\n    qml.RX(weights[0], 0)\n    qml.RY(weights[1], 1)\n    return qml.expval(qml.PauliZ(0))\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc3(weights, x):\n    for i in range(2):\n        qml.Hadamard(i)\n        qml.RX(x[i], i)\n        qml.RY(weights[i], i)\n        qml.RZ(weights[i], i)\n    qml.CNOT([0,1])\n    return qml.expval(qml.PauliZ(1))\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc4(weights, x):\n    qml.Hadamard(0)\n    qml.CNOT([0,1])\n    qml.RX(x[0], 0)\n    qml.RY(x[1], 1)\n    qml.CNOT([1,0])\n    qml.RZ(weights[0], 0)\n    qml.RZ(weights[1], 1)\n    return qml.expval(qml.PauliZ(0))\n\nQUANTUM_CIRCUITS = {\n    \"QC1\": qc1,\n    \"QC2\": qc2,\n    \"QC3\": qc3,\n    \"QC4\": qc4\n}\nclass QBiLSTM(nn.Module):\n    def __init__(self, quantum_circuit):\n        super().__init__()\n        self.qc = quantum_circuit\n        self.q_weights = nn.Parameter(torch.randn(2))\n\n        self.encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n\n        self.bilstm = nn.LSTM(\n            input_size=768,\n            hidden_size=128,\n            bidirectional=True,\n            batch_first=True\n        )\n\n        self.fc = nn.Linear(256, 2)\n\n    def quantum_layer(self, x):\n        q_outs = []\n        for v in x:\n            q_outs.append(self.qc(self.q_weights, v[:2]))\n        return torch.stack(q_outs)\n\n    def forward(self, input_ids, attention_mask):\n        enc = self.encoder(input_ids, attention_mask).last_hidden_state\n        lstm_out, _ = self.bilstm(enc)\n        h = lstm_out[:, -1, :]\n\n        _ = self.quantum_layer(h)  # quantum interaction\n\n        return self.fc(h)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresults = {}\n\nfor name, qc in QUANTUM_CIRCUITS.items():\n    print(f\"\\n====== Training {name} ======\")\n\n    model = QBiLSTM(qc).to(device)\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(10):\n        print(f\"\\n--- Epoch {epoch+1}/10 ---\")\n        model.train()\n        total_loss = 0\n\n        for step, batch in enumerate(train_loader, 1):\n            ids, mask, y = [b.to(device) for b in batch]\n\n            optimizer.zero_grad()\n            out = model(ids, mask)\n            loss = loss_fn(out, y)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            if step % 50 == 0:  # print every 50 steps\n                print(f\"Step {step}: Loss = {loss.item():.4f}\")\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1} finished. Avg Loss = {avg_loss:.4f}\")\n\n    # Evaluation\n    model.eval()\n    preds, gold = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            ids, mask, y = [b.to(device) for b in batch]\n            logits = model(ids, mask)\n            preds.extend(torch.argmax(logits, 1).cpu().numpy())\n            gold.extend(y.cpu().numpy())\n\n    acc = accuracy_score(gold, preds)\n    results[name] = acc\n\n    print(f\"{name} Accuracy: {acc:.4f}\")\n    print(classification_report(gold, preds))\n\nprint(\"\\n===== FINAL QUANTUM CIRCUIT COMPARISON =====\")\nfor k, v in results.items():\n    print(f\"{k}: {v:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T05:53:03.603422Z","iopub.execute_input":"2026-02-12T05:53:03.603767Z","iopub.status.idle":"2026-02-12T06:19:08.358700Z","shell.execute_reply.started":"2026-02-12T05:53:03.603744Z","shell.execute_reply":"2026-02-12T06:19:08.358031Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"                                  sentence  label\n0         would not find it at all strange      0\n1  that four shots would solve the problem      0\n2                                  in turn      0\n3                                  because      0\n4                                 regained      0\n(8603, 2)\n                                  sentence  label  \\\n0         would not find it at all strange      0   \n1  that four shots would solve the problem      0   \n2                                  in turn      0   \n3                                  because      0   \n4                                 regained      0   \n\n                     cleaned_text  \n0              would find strange  \n1  four shots would solve problem  \n2                            turn  \n3                                  \n4                        regained  \nlabel\n0    6292\n1    2311\nName: count, dtype: int64\nBalanced Class Distribution:\nlabel\n0    2311\n1    2311\nName: count, dtype: int64\nlabel\n0    2311\n1    2311\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"\n====== Training QC1 ======\n\n--- Epoch 1/10 ---\nStep 50: Loss = 0.6426\nStep 100: Loss = 0.4824\nStep 150: Loss = 0.4929\nStep 200: Loss = 0.2422\nEpoch 1 finished. Avg Loss = 0.4499\n\n--- Epoch 2/10 ---\nStep 50: Loss = 0.1202\nStep 100: Loss = 0.2069\nStep 150: Loss = 0.2803\nStep 200: Loss = 0.2999\nEpoch 2 finished. Avg Loss = 0.3025\n\n--- Epoch 3/10 ---\nStep 50: Loss = 0.1300\nStep 100: Loss = 0.0414\nStep 150: Loss = 0.2195\nStep 200: Loss = 0.0860\nEpoch 3 finished. Avg Loss = 0.2204\n\n--- Epoch 4/10 ---\nStep 50: Loss = 0.1577\nStep 100: Loss = 0.1735\nStep 150: Loss = 0.0222\nStep 200: Loss = 0.1640\nEpoch 4 finished. Avg Loss = 0.1535\n\n--- Epoch 5/10 ---\nStep 50: Loss = 0.0218\nStep 100: Loss = 0.0905\nStep 150: Loss = 0.0550\nStep 200: Loss = 0.0589\nEpoch 5 finished. Avg Loss = 0.1258\n\n--- Epoch 6/10 ---\nStep 50: Loss = 0.1541\nStep 100: Loss = 0.0323\nStep 150: Loss = 0.0199\nStep 200: Loss = 0.0318\nEpoch 6 finished. Avg Loss = 0.1041\n\n--- Epoch 7/10 ---\nStep 50: Loss = 0.1894\nStep 100: Loss = 0.0479\nStep 150: Loss = 0.0215\nStep 200: Loss = 0.0516\nEpoch 7 finished. Avg Loss = 0.0895\n\n--- Epoch 8/10 ---\nStep 50: Loss = 0.0780\nStep 100: Loss = 0.0769\nStep 150: Loss = 0.0206\nStep 200: Loss = 0.0973\nEpoch 8 finished. Avg Loss = 0.0802\n\n--- Epoch 9/10 ---\nStep 50: Loss = 0.0293\nStep 100: Loss = 0.0060\nStep 150: Loss = 0.0116\nStep 200: Loss = 0.0817\nEpoch 9 finished. Avg Loss = 0.0703\n\n--- Epoch 10/10 ---\nStep 50: Loss = 0.0463\nStep 100: Loss = 0.0192\nStep 150: Loss = 0.1076\nStep 200: Loss = 0.0066\nEpoch 10 finished. Avg Loss = 0.0805\nQC1 Accuracy: 0.8497\n              precision    recall  f1-score   support\n\n           0       0.89      0.80      0.84       459\n           1       0.82      0.90      0.86       466\n\n    accuracy                           0.85       925\n   macro avg       0.85      0.85      0.85       925\nweighted avg       0.85      0.85      0.85       925\n\n\n====== Training QC2 ======\n\n--- Epoch 1/10 ---\nStep 50: Loss = 0.4677\nStep 100: Loss = 0.4856\nStep 150: Loss = 0.3901\nStep 200: Loss = 0.2123\nEpoch 1 finished. Avg Loss = 0.4470\n\n--- Epoch 2/10 ---\nStep 50: Loss = 0.1505\nStep 100: Loss = 0.1846\nStep 150: Loss = 0.2447\nStep 200: Loss = 0.1367\nEpoch 2 finished. Avg Loss = 0.2925\n\n--- Epoch 3/10 ---\nStep 50: Loss = 0.2025\nStep 100: Loss = 0.1351\nStep 150: Loss = 0.1182\nStep 200: Loss = 0.2728\nEpoch 3 finished. Avg Loss = 0.2074\n\n--- Epoch 4/10 ---\nStep 50: Loss = 0.0267\nStep 100: Loss = 0.0463\nStep 150: Loss = 0.0605\nStep 200: Loss = 0.3703\nEpoch 4 finished. Avg Loss = 0.1527\n\n--- Epoch 5/10 ---\nStep 50: Loss = 0.0314\nStep 100: Loss = 0.1056\nStep 150: Loss = 0.0281\nStep 200: Loss = 0.3328\nEpoch 5 finished. Avg Loss = 0.1233\n\n--- Epoch 6/10 ---\nStep 50: Loss = 0.1172\nStep 100: Loss = 0.1953\nStep 150: Loss = 0.1073\nStep 200: Loss = 0.0152\nEpoch 6 finished. Avg Loss = 0.0986\n\n--- Epoch 7/10 ---\nStep 50: Loss = 0.1169\nStep 100: Loss = 0.0724\nStep 150: Loss = 0.0201\nStep 200: Loss = 0.0243\nEpoch 7 finished. Avg Loss = 0.0927\n\n--- Epoch 8/10 ---\nStep 50: Loss = 0.0219\nStep 100: Loss = 0.1133\nStep 150: Loss = 0.0068\nStep 200: Loss = 0.2430\nEpoch 8 finished. Avg Loss = 0.0862\n\n--- Epoch 9/10 ---\nStep 50: Loss = 0.0454\nStep 100: Loss = 0.0325\nStep 150: Loss = 0.1184\nStep 200: Loss = 0.0760\nEpoch 9 finished. Avg Loss = 0.0778\n\n--- Epoch 10/10 ---\nStep 50: Loss = 0.0887\nStep 100: Loss = 0.1358\nStep 150: Loss = 0.1524\nStep 200: Loss = 0.0755\nEpoch 10 finished. Avg Loss = 0.0795\nQC2 Accuracy: 0.8443\n              precision    recall  f1-score   support\n\n           0       0.86      0.82      0.84       459\n           1       0.83      0.87      0.85       466\n\n    accuracy                           0.84       925\n   macro avg       0.85      0.84      0.84       925\nweighted avg       0.85      0.84      0.84       925\n\n\n====== Training QC3 ======\n\n--- Epoch 1/10 ---\nStep 50: Loss = 0.8218\nStep 100: Loss = 0.2427\nStep 150: Loss = 0.3516\nStep 200: Loss = 0.3072\nEpoch 1 finished. Avg Loss = 0.4179\n\n--- Epoch 2/10 ---\nStep 50: Loss = 0.2657\nStep 100: Loss = 0.1082\nStep 150: Loss = 0.1470\nStep 200: Loss = 0.1182\nEpoch 2 finished. Avg Loss = 0.2746\n\n--- Epoch 3/10 ---\nStep 50: Loss = 0.0917\nStep 100: Loss = 0.5045\nStep 150: Loss = 0.0820\nStep 200: Loss = 0.0938\nEpoch 3 finished. Avg Loss = 0.2092\n\n--- Epoch 4/10 ---\nStep 50: Loss = 0.0241\nStep 100: Loss = 0.0999\nStep 150: Loss = 0.2032\nStep 200: Loss = 0.1457\nEpoch 4 finished. Avg Loss = 0.1503\n\n--- Epoch 5/10 ---\nStep 50: Loss = 0.0502\nStep 100: Loss = 0.0475\nStep 150: Loss = 0.0856\nStep 200: Loss = 0.0522\nEpoch 5 finished. Avg Loss = 0.1112\n\n--- Epoch 6/10 ---\nStep 50: Loss = 0.0350\nStep 100: Loss = 0.1826\nStep 150: Loss = 0.0557\nStep 200: Loss = 0.0916\nEpoch 6 finished. Avg Loss = 0.1066\n\n--- Epoch 7/10 ---\nStep 50: Loss = 0.0154\nStep 100: Loss = 0.0584\nStep 150: Loss = 0.0103\nStep 200: Loss = 0.2537\nEpoch 7 finished. Avg Loss = 0.0915\n\n--- Epoch 8/10 ---\nStep 50: Loss = 0.3753\nStep 100: Loss = 0.0047\nStep 150: Loss = 0.0470\nStep 200: Loss = 0.1289\nEpoch 8 finished. Avg Loss = 0.0833\n\n--- Epoch 9/10 ---\nStep 50: Loss = 0.0054\nStep 100: Loss = 0.0562\nStep 150: Loss = 0.2402\nStep 200: Loss = 0.0847\nEpoch 9 finished. Avg Loss = 0.0864\n\n--- Epoch 10/10 ---\nStep 50: Loss = 0.0599\nStep 100: Loss = 0.0049\nStep 150: Loss = 0.0158\nStep 200: Loss = 0.0873\nEpoch 10 finished. Avg Loss = 0.0711\nQC3 Accuracy: 0.8562\n              precision    recall  f1-score   support\n\n           0       0.87      0.83      0.85       459\n           1       0.84      0.88      0.86       466\n\n    accuracy                           0.86       925\n   macro avg       0.86      0.86      0.86       925\nweighted avg       0.86      0.86      0.86       925\n\n\n====== Training QC4 ======\n\n--- Epoch 1/10 ---\nStep 50: Loss = 0.4432\nStep 100: Loss = 0.3390\nStep 150: Loss = 0.3216\nStep 200: Loss = 0.5375\nEpoch 1 finished. Avg Loss = 0.4419\n\n--- Epoch 2/10 ---\nStep 50: Loss = 0.4091\nStep 100: Loss = 0.4445\nStep 150: Loss = 0.2837\nStep 200: Loss = 0.1738\nEpoch 2 finished. Avg Loss = 0.2854\n\n--- Epoch 3/10 ---\nStep 50: Loss = 0.0657\nStep 100: Loss = 0.4792\nStep 150: Loss = 0.1052\nStep 200: Loss = 0.2948\nEpoch 3 finished. Avg Loss = 0.2126\n\n--- Epoch 4/10 ---\nStep 50: Loss = 0.3166\nStep 100: Loss = 0.3350\nStep 150: Loss = 0.4449\nStep 200: Loss = 0.0332\nEpoch 4 finished. Avg Loss = 0.1576\n\n--- Epoch 5/10 ---\nStep 50: Loss = 0.0321\nStep 100: Loss = 0.0315\nStep 150: Loss = 0.1074\nStep 200: Loss = 0.0688\nEpoch 5 finished. Avg Loss = 0.1241\n\n--- Epoch 6/10 ---\nStep 50: Loss = 0.0120\nStep 100: Loss = 0.0164\nStep 150: Loss = 0.0216\nStep 200: Loss = 0.1228\nEpoch 6 finished. Avg Loss = 0.1017\n\n--- Epoch 7/10 ---\nStep 50: Loss = 0.0117\nStep 100: Loss = 0.0204\nStep 150: Loss = 0.0776\nStep 200: Loss = 0.0726\nEpoch 7 finished. Avg Loss = 0.0910\n\n--- Epoch 8/10 ---\nStep 50: Loss = 0.0756\nStep 100: Loss = 0.0074\nStep 150: Loss = 0.0209\nStep 200: Loss = 0.0089\nEpoch 8 finished. Avg Loss = 0.0936\n\n--- Epoch 9/10 ---\nStep 50: Loss = 0.0400\nStep 100: Loss = 0.0145\nStep 150: Loss = 0.4720\nStep 200: Loss = 0.0548\nEpoch 9 finished. Avg Loss = 0.0740\n\n--- Epoch 10/10 ---\nStep 50: Loss = 0.1663\nStep 100: Loss = 0.0188\nStep 150: Loss = 0.0847\nStep 200: Loss = 0.1062\nEpoch 10 finished. Avg Loss = 0.0647\nQC4 Accuracy: 0.8346\n              precision    recall  f1-score   support\n\n           0       0.81      0.86      0.84       459\n           1       0.86      0.81      0.83       466\n\n    accuracy                           0.83       925\n   macro avg       0.84      0.83      0.83       925\nweighted avg       0.84      0.83      0.83       925\n\n\n===== FINAL QUANTUM CIRCUIT COMPARISON =====\nQC1: 0.8497\nQC2: 0.8443\nQC3: 0.8562\nQC4: 0.8346\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# **DeBARTA**","metadata":{}},{"cell_type":"code","source":"!pip install pennylane\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport torch\nfrom sklearn.metrics import (\n    roc_curve, roc_auc_score, classification_report, confusion_matrix\n)\nfrom sklearn.preprocessing import label_binarize\nimport pennylane as qml\nfrom pennylane import numpy as pnp\nimport re\nimport warnings\nimport re\nimport nltk\n\nfrom nltk.corpus import stopwords\nwarnings.filterwarnings('ignore')\n# Label encoding for 'status' column\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\nimport pennylane as qml\n# Convert data to PyTorch DataLoader format\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, roc_curve, auc\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import ParameterSampler\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport numpy as np\nimport torch\nfrom sklearn.preprocessing import label_binarize\nimport time\nimport random\nimport numpy as np\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModel\n\nds = load_dataset(\"jxm/mpqa\")\n\n# Convert train split to pandas (MPQA already split)\ndf = ds[\"train\"].to_pandas()\n\n\ndef preprocess_text(text):\n    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove links\n    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = text.lower()  # Convert to lowercase\n    return text\n\ndf['cleaned_text'] = df['sentence'].apply(preprocess_text)\n\ndf = df[df['cleaned_text'] != '[deleted]']\ndf = df[df['cleaned_text'] != '[removed]']\n\ndf.head()\n\n# Download NLTK stopwords (only needed once)\nnltk.download('stopwords')\n\n# Get English stopwords from NLTK\nstop_words = set(stopwords.words('english'))\n\ndef clean_statement(statement):\n    # Convert to lowercase\n    statement = statement.lower()\n\n    # Remove special characters (punctuation, non-alphabetic characters)\n    statement = re.sub(r'[^\\w\\s]', '', statement)\n\n    # Remove numbers (optional, depending on your use case)\n    statement = re.sub(r'\\d+', '', statement)\n\n    # Tokenize the statement (split into words)\n    words = statement.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in stop_words]\n\n    # Rejoin words into a cleaned statement\n    cleaned_statement = ' '.join(words)\n\n    return cleaned_statement\n\n# Apply the cleaning function to the 'statement' column\ndf['cleaned_text'] = df['cleaned_text'].apply(clean_statement)\n\ndf\n\n\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['label'])\n\ndf.head()\n\n# Check the number of samples per class\nclass_counts = df['label'].value_counts()\nprint(class_counts)\n# Separate majority and minority classes\nclass_counts = df['label'].value_counts()\nprint(class_counts)\n\nmajority_class = df['label'].value_counts().idxmax()\nminority_class = df['label'].value_counts().idxmin()\n\ndf_majority = df[df['label'] == majority_class]\ndf_minority = df[df['label'] == minority_class]\n\n# Downsample majority to match minority\ndf_majority_downsampled = df_majority.sample(len(df_minority), random_state=42)\n\n# Combine\ndf_balanced = pd.concat([df_majority_downsampled, df_minority])\n\n# Shuffle\ndf_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(df_balanced['label'].value_counts())\n\n\n# Initialize the tokenizer and model from Hugging Face\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n\ntokens = tokenizer.batch_encode_plus(\n    df['cleaned_text'].tolist(),\n    max_length=128,\n    padding=\"max_length\",   # <-- updated\n    truncation=True,\n    return_tensors=\"pt\"\n)\n\n# Convert to tensors\ninput_ids = torch.tensor(tokens['input_ids'])\nattention_masks = torch.tensor(tokens['attention_mask'])\nlabels = torch.tensor(df['label'].values)\n\n# Split the data into training and validation sets\ntrain_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.2)\ntrain_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.2)\n\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\n\ntrain_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=32)\nval_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=32)\n\n# Create a quantum node (circuit)\ndev = qml.device(\"default.qubit\", wires=2)  # or 'default.qubit'\n\n@qml.qnode(dev)\ndef quantum_circuit(weights, inputs):\n    qml.Hadamard(wires=0)\n    qml.Hadamard(wires=1)\n\n    qml.RX(inputs[0], wires=0)\n    qml.RY(inputs[1], wires=1)\n\n    qml.CNOT(wires=[0, 1])\n\n    qml.RZ(weights[0], wires=0)\n    qml.RZ(weights[1], wires=1)\n\n    return qml.expval(qml.PauliZ(0))\n\nclass QBiLSTM(nn.Module):\n    def __init__(self):\n        super(QBiLSTM, self).__init__()\n\n        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n\n        # ðŸ” BiLSTM instead of LSTM\n        self.bilstm = nn.LSTM(\n            input_size=768,\n            hidden_size=128,\n            batch_first=True,\n            bidirectional=True\n        )\n\n        # 128 Ã— 2 because BiLSTM\n        self.fc = nn.Linear(256, 2)\n\n    def quantum_layer(self, inputs):\n        processed_features = []\n\n        for feature_vector in inputs:\n            features_for_quantum = feature_vector[:2]\n            q_out = quantum_circuit(\n                torch.randn(2, dtype=torch.float32),\n                features_for_quantum\n            )\n            processed_features.append(q_out)\n\n        return torch.stack(processed_features).unsqueeze(1)\n\n    def forward(self, input_ids, attention_mask):\n        bert_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # BiLSTM output\n        lstm_output, _ = self.bilstm(bert_output.last_hidden_state)\n\n        # Last timestep (batch, 256)\n        last_hidden = lstm_output[:, -1, :]\n\n        _ = self.quantum_layer(last_hidden)\n\n        # Classification\n        output = self.fc(last_hidden)\n        return output\n\n\nmodel = QBiLSTM()\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define hyperparameter search space\nparam_grid = {\n    'learning_rate': [1e-5, 2e-5, 3e-5, 5e-5],\n    'batch_size': [16, 32],\n    'epochs': [3, 5, 7]\n}\n\nnum_samples = 5  \nparam_list = list(ParameterSampler(param_grid, n_iter=num_samples, random_state=42))\n\nbest_model = None\nbest_val_accuracy = 0.0\nbest_params = None\n\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Loop through each random set of hyperparameters\nfor idx, params in enumerate(param_list):\n    print(f\"Testing configuration {idx + 1}: {params}\")\n\n    learning_rate = params['learning_rate']\n    batch_size = params['batch_size']\n    epochs = params['epochs']\n\n    # Define optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n    for epoch in range(epochs):\n        start_time = time.time()\n        model.train()\n        total_train_loss = 0\n        total_train_accuracy = 0\n\n        for batch in train_dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n\n            # Move tensors to the same device\n            b_input_ids = b_input_ids.to(device)\n            b_input_mask = b_input_mask.to(device)\n            b_labels = b_labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(b_input_ids, b_input_mask)\n            loss = nn.CrossEntropyLoss()(outputs, b_labels)\n            total_train_loss += loss.item()\n\n            logits = outputs.detach().cpu().numpy()\n            label_ids = b_labels.cpu().numpy()\n            total_train_accuracy += flat_accuracy(logits, label_ids)\n\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n        avg_train_loss = total_train_loss / len(train_dataloader)\n        avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n\n        # Validation\n        model.eval()\n        total_val_loss = 0\n        total_val_accuracy = 0\n        all_preds = []\n        all_labels = []\n\n        for batch in val_dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n\n            # Move tensors to the same device\n            b_input_ids = b_input_ids.to(device)\n            b_input_mask = b_input_mask.to(device)\n            b_labels = b_labels.to(device)\n\n            with torch.no_grad():\n                outputs = model(b_input_ids, b_input_mask)\n                loss = nn.CrossEntropyLoss()(outputs, b_labels)\n                total_val_loss += loss.item()\n\n                logits = outputs.detach().cpu().numpy()\n                label_ids = b_labels.cpu().numpy()\n                total_val_accuracy += flat_accuracy(logits, label_ids)\n\n                all_preds.extend(np.argmax(logits, axis=1).flatten())\n                all_labels.extend(label_ids.flatten())\n\n        avg_val_loss = total_val_loss / len(val_dataloader)\n        avg_val_accuracy = total_val_accuracy / len(val_dataloader)\n        elapsed_time = time.time() - start_time\n\n        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Train Accuracy: {avg_train_accuracy:.4f}\")\n        print(f\"Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {avg_val_accuracy:.4f}\")\n        print(f\"Time: {elapsed_time:.2f} seconds\")\n\n        # Save best model\n        if avg_val_accuracy > best_val_accuracy:\n            best_val_accuracy = avg_val_accuracy\n            best_model = model.state_dict()\n            best_params = params\n            torch.save(best_model, 'best_model.pth')\n            print(f\"New best model saved with accuracy: {best_val_accuracy:.4f}\")\n\n# Generate classification report\nprint(\"Classification Report:\")\nprint(classification_report(all_labels, all_preds))\n\n\n\n# Define search space for hyperparameters\nepochs = 5\nlearning_rates = [2e-5, 3e-5, 5e-5]\nbatch_sizes = [16, 32]\noptimizers = ['adamw', 'adam', 'rmsprop', 'sgd']\n\n# Number of random samples to try\nnum_samples = 5\n\n# Function to calculate accuracy\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Function to get optimizer\ndef get_optimizer(optimizer_name, model_parameters, lr):\n    if optimizer_name == 'adamw':\n        return optim.AdamW(model_parameters, lr=lr)\n    elif optimizer_name == 'adam':\n        return optim.Adam(model_parameters, lr=lr)\n    elif optimizer_name == 'rmsprop':\n        return optim.RMSprop(model_parameters, lr=lr)\n    elif optimizer_name == 'sgd':\n        return optim.SGD(model_parameters, lr=lr)\n\n# Randomly sample hyperparameter combinations\nrandom_hyperparams = [\n    {\n        \"optimizer\": random.choice(optimizers),\n        \"learning_rate\": random.choice(learning_rates),\n        \"batch_size\": random.choice(batch_sizes),\n    }\n    for _ in range(num_samples)\n]\n\nbest_accuracy = 0\nbest_params = {}\n\n# Iterate over randomly chosen hyperparameter sets\nfor params in random_hyperparams:\n    optimizer_name = params[\"optimizer\"]\n    lr = params[\"learning_rate\"]\n    batch_size = params[\"batch_size\"]\n\n    # Initialize data loaders\n    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n    val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n\n    # Define optimizer and scheduler\n    optimizer = get_optimizer(optimizer_name, model.parameters(), lr)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n    # Move model to device\n    model.to(device)\n\n    for epoch in range(epochs):\n        start_time = time.time()\n\n        # Training\n        model.train()\n        total_train_loss = 0\n        total_train_accuracy = 0\n        for batch in train_dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(b_input_ids, b_input_mask)\n            loss = nn.CrossEntropyLoss()(outputs, b_labels)\n            total_train_loss += loss.item()\n\n            logits = outputs.detach().cpu().numpy()\n            label_ids = b_labels.cpu().numpy()\n            total_train_accuracy += flat_accuracy(logits, label_ids)\n\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n        avg_train_loss = total_train_loss / len(train_dataloader)\n        avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n\n        # Validation\n        model.eval()\n        total_val_loss = 0\n        total_val_accuracy = 0\n        for batch in val_dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n\n            with torch.no_grad():\n                outputs = model(b_input_ids, b_input_mask)\n                loss = nn.CrossEntropyLoss()(outputs, b_labels)\n                total_val_loss += loss.item()\n\n                logits = outputs.detach().cpu().numpy()\n                label_ids = b_labels.cpu().numpy()\n                total_val_accuracy += flat_accuracy(logits, label_ids)\n\n        avg_val_loss = total_val_loss / len(val_dataloader)\n        avg_val_accuracy = total_val_accuracy / len(val_dataloader)\n\n        elapsed_time = time.time() - start_time\n\n        print(f\"Optimizer: {optimizer_name} | Learning Rate: {lr} | Batch Size: {batch_size}\")\n        print(f\"Epoch {epoch+1}\")\n        print(f\"Train Loss: {avg_train_loss:.4f} | Train Accuracy: {avg_train_accuracy:.4f}\")\n        print(f\"Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {avg_val_accuracy:.4f}\")\n        print(f\"Time: {elapsed_time:.2f} seconds\")\n        print(\"-\" * 50)\n\n        # Update best parameters if validation accuracy improves\n        if avg_val_accuracy > best_accuracy:\n            best_accuracy = avg_val_accuracy\n            best_params = {\"optimizer\": optimizer_name, \"learning_rate\": lr, \"batch_size\": batch_size}\n\nprint(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\nprint(f\"Best Parameters: Optimizer = {best_params['optimizer']}, Learning Rate = {best_params['learning_rate']}, Batch Size = {best_params['batch_size']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T04:20:52.328412Z","iopub.execute_input":"2026-02-14T04:20:52.329595Z","iopub.status.idle":"2026-02-14T06:27:08.255292Z","shell.execute_reply.started":"2026-02-14T04:20:52.329563Z","shell.execute_reply":"2026-02-14T06:27:08.254592Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pennylane in /usr/local/lib/python3.12/dist-packages (0.44.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.15.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\nRequirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.17.1)\nRequirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\nRequirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.4.4)\nRequirement already satisfied: autoray==0.8.2 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.8.2)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\nRequirement already satisfied: pennylane-lightning>=0.44 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.44.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.5)\nRequirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (26.0rc2)\nRequirement already satisfied: diastatic-malt in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.15.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\nRequirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning>=0.44->pennylane) (0.3.31.22.1)\nRequirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\nRequirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2026.1.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\nRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\nlabel\n0    6292\n1    2311\nName: count, dtype: int64\nlabel\n0    6292\n1    2311\nName: count, dtype: int64\nlabel\n0    2311\n1    2311\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4c6a6a3ae204ca6a37d7e60faaf22bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ced774907baa43d9b16d41f21d68d73a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41205f3af11e449f8175eddfa81f22f4"}},"metadata":{}},{"name":"stderr","text":"2026-02-14 04:21:02.232163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771042862.382792      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771042862.423904      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771042862.790927      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771042862.790968      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771042862.790971      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771042862.790973      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"479597b0432f4ab3b39fa41c02716b7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"596e5f42b0f848e48b032e594197d335"}},"metadata":{}},{"name":"stdout","text":"Testing configuration 1: {'learning_rate': 1e-05, 'epochs': 7, 'batch_size': 16}\nEpoch 1 | Train Loss: 0.5811 | Train Accuracy: 0.7347\nValidation Loss: 0.6042 | Validation Accuracy: 0.7192\nTime: 145.72 seconds\nNew best model saved with accuracy: 0.7192\nEpoch 2 | Train Loss: 0.5808 | Train Accuracy: 0.7355\nValidation Loss: 0.5831 | Validation Accuracy: 0.7192\nTime: 144.14 seconds\nEpoch 3 | Train Loss: 0.5778 | Train Accuracy: 0.7355\nValidation Loss: 0.5715 | Validation Accuracy: 0.7192\nTime: 144.56 seconds\nEpoch 4 | Train Loss: 0.5646 | Train Accuracy: 0.7357\nValidation Loss: 0.5962 | Validation Accuracy: 0.7192\nTime: 144.29 seconds\nEpoch 5 | Train Loss: 0.5777 | Train Accuracy: 0.7355\nValidation Loss: 0.5936 | Validation Accuracy: 0.7192\nTime: 144.08 seconds\nEpoch 6 | Train Loss: 0.5716 | Train Accuracy: 0.7334\nValidation Loss: 0.5927 | Validation Accuracy: 0.7192\nTime: 144.36 seconds\nEpoch 7 | Train Loss: 0.5112 | Train Accuracy: 0.7355\nValidation Loss: 0.5892 | Validation Accuracy: 0.7192\nTime: 144.01 seconds\nTesting configuration 2: {'learning_rate': 1e-05, 'epochs': 5, 'batch_size': 32}\nEpoch 1 | Train Loss: 0.4968 | Train Accuracy: 0.7422\nValidation Loss: 0.4370 | Validation Accuracy: 0.7267\nTime: 143.99 seconds\nNew best model saved with accuracy: 0.7267\nEpoch 2 | Train Loss: 0.3724 | Train Accuracy: 0.8423\nValidation Loss: 0.3762 | Validation Accuracy: 0.8541\nTime: 143.82 seconds\nNew best model saved with accuracy: 0.8541\nEpoch 3 | Train Loss: 0.3350 | Train Accuracy: 0.8655\nValidation Loss: 0.3496 | Validation Accuracy: 0.8713\nTime: 143.95 seconds\nNew best model saved with accuracy: 0.8713\nEpoch 4 | Train Loss: 0.2994 | Train Accuracy: 0.8867\nValidation Loss: 0.3344 | Validation Accuracy: 0.8755\nTime: 143.76 seconds\nNew best model saved with accuracy: 0.8755\nEpoch 5 | Train Loss: 0.2813 | Train Accuracy: 0.8948\nValidation Loss: 0.3317 | Validation Accuracy: 0.8738\nTime: 144.04 seconds\nTesting configuration 3: {'learning_rate': 1e-05, 'epochs': 3, 'batch_size': 16}\nEpoch 1 | Train Loss: 0.2988 | Train Accuracy: 0.8818\nValidation Loss: 0.3239 | Validation Accuracy: 0.8759\nTime: 143.77 seconds\nNew best model saved with accuracy: 0.8759\nEpoch 2 | Train Loss: 0.2690 | Train Accuracy: 0.9028\nValidation Loss: 0.3294 | Validation Accuracy: 0.8759\nTime: 143.69 seconds\nEpoch 3 | Train Loss: 0.2393 | Train Accuracy: 0.9129\nValidation Loss: 0.3264 | Validation Accuracy: 0.8794\nTime: 144.42 seconds\nNew best model saved with accuracy: 0.8794\nTesting configuration 4: {'learning_rate': 3e-05, 'epochs': 5, 'batch_size': 32}\nEpoch 1 | Train Loss: 0.3047 | Train Accuracy: 0.8853\nValidation Loss: 0.3224 | Validation Accuracy: 0.8742\nTime: 144.78 seconds\nEpoch 2 | Train Loss: 0.2487 | Train Accuracy: 0.9116\nValidation Loss: 0.3242 | Validation Accuracy: 0.8823\nTime: 144.19 seconds\nNew best model saved with accuracy: 0.8823\nEpoch 3 | Train Loss: 0.1994 | Train Accuracy: 0.9303\nValidation Loss: 0.3342 | Validation Accuracy: 0.8836\nTime: 144.38 seconds\nNew best model saved with accuracy: 0.8836\nEpoch 4 | Train Loss: 0.1825 | Train Accuracy: 0.9379\nValidation Loss: 0.3570 | Validation Accuracy: 0.8830\nTime: 144.22 seconds\nEpoch 5 | Train Loss: 0.1775 | Train Accuracy: 0.9410\nValidation Loss: 0.3564 | Validation Accuracy: 0.8788\nTime: 144.30 seconds\nTesting configuration 5: {'learning_rate': 5e-05, 'epochs': 7, 'batch_size': 16}\nEpoch 1 | Train Loss: 0.2174 | Train Accuracy: 0.9214\nValidation Loss: 0.3480 | Validation Accuracy: 0.8748\nTime: 143.69 seconds\nEpoch 2 | Train Loss: 0.1907 | Train Accuracy: 0.9329\nValidation Loss: 0.3760 | Validation Accuracy: 0.8709\nTime: 143.37 seconds\nEpoch 3 | Train Loss: 0.1738 | Train Accuracy: 0.9391\nValidation Loss: 0.3502 | Validation Accuracy: 0.8759\nTime: 143.32 seconds\nEpoch 4 | Train Loss: 0.1585 | Train Accuracy: 0.9456\nValidation Loss: 0.3586 | Validation Accuracy: 0.8725\nTime: 143.69 seconds\nEpoch 5 | Train Loss: 0.1449 | Train Accuracy: 0.9524\nValidation Loss: 0.4003 | Validation Accuracy: 0.8736\nTime: 144.12 seconds\nEpoch 6 | Train Loss: 0.1275 | Train Accuracy: 0.9582\nValidation Loss: 0.4153 | Validation Accuracy: 0.8730\nTime: 144.05 seconds\nEpoch 7 | Train Loss: 0.1155 | Train Accuracy: 0.9637\nValidation Loss: 0.4296 | Validation Accuracy: 0.8771\nTime: 143.98 seconds\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.92      0.92      1238\n           1       0.79      0.77      0.78       483\n\n    accuracy                           0.88      1721\n   macro avg       0.85      0.84      0.85      1721\nweighted avg       0.88      0.88      0.88      1721\n\nOptimizer: rmsprop | Learning Rate: 5e-05 | Batch Size: 16\nEpoch 1\nTrain Loss: 0.5874 | Train Accuracy: 0.7314\nValidation Loss: 0.5938 | Validation Accuracy: 0.7191\nTime: 151.87 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 5e-05 | Batch Size: 16\nEpoch 2\nTrain Loss: 0.5822 | Train Accuracy: 0.7329\nValidation Loss: 0.5950 | Validation Accuracy: 0.7191\nTime: 151.33 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 5e-05 | Batch Size: 16\nEpoch 3\nTrain Loss: 0.5862 | Train Accuracy: 0.7325\nValidation Loss: 0.6420 | Validation Accuracy: 0.6400\nTime: 153.16 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 5e-05 | Batch Size: 16\nEpoch 4\nTrain Loss: 0.4192 | Train Accuracy: 0.8173\nValidation Loss: 0.4989 | Validation Accuracy: 0.7541\nTime: 151.88 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 5e-05 | Batch Size: 16\nEpoch 5\nTrain Loss: 0.2830 | Train Accuracy: 0.8866\nValidation Loss: 0.5056 | Validation Accuracy: 0.7712\nTime: 151.51 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 1\nTrain Loss: 0.2511 | Train Accuracy: 0.9040\nValidation Loss: 0.5034 | Validation Accuracy: 0.7758\nTime: 144.93 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 2\nTrain Loss: 0.2531 | Train Accuracy: 0.9036\nValidation Loss: 0.5003 | Validation Accuracy: 0.7753\nTime: 144.73 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 3\nTrain Loss: 0.2481 | Train Accuracy: 0.9085\nValidation Loss: 0.4994 | Validation Accuracy: 0.7753\nTime: 144.92 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 4\nTrain Loss: 0.2489 | Train Accuracy: 0.9049\nValidation Loss: 0.5039 | Validation Accuracy: 0.7741\nTime: 145.12 seconds\n--------------------------------------------------\nOptimizer: sgd | Learning Rate: 3e-05 | Batch Size: 16\nEpoch 5\nTrain Loss: 0.2479 | Train Accuracy: 0.9044\nValidation Loss: 0.5065 | Validation Accuracy: 0.7730\nTime: 144.94 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 1\nTrain Loss: 0.2813 | Train Accuracy: 0.8927\nValidation Loss: 0.4184 | Validation Accuracy: 0.8454\nTime: 143.36 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 2\nTrain Loss: 0.2196 | Train Accuracy: 0.9172\nValidation Loss: 0.4164 | Validation Accuracy: 0.8686\nTime: 142.28 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 3\nTrain Loss: 0.1887 | Train Accuracy: 0.9348\nValidation Loss: 0.4369 | Validation Accuracy: 0.8622\nTime: 142.77 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 4\nTrain Loss: 0.1724 | Train Accuracy: 0.9442\nValidation Loss: 0.4371 | Validation Accuracy: 0.8645\nTime: 142.24 seconds\n--------------------------------------------------\nOptimizer: adam | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 5\nTrain Loss: 0.1576 | Train Accuracy: 0.9484\nValidation Loss: 0.4397 | Validation Accuracy: 0.8634\nTime: 142.08 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 1\nTrain Loss: 0.3019 | Train Accuracy: 0.8947\nValidation Loss: 0.4187 | Validation Accuracy: 0.8574\nTime: 149.49 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 2\nTrain Loss: 0.2838 | Train Accuracy: 0.8888\nValidation Loss: 0.4239 | Validation Accuracy: 0.8323\nTime: 149.47 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 3\nTrain Loss: 0.2302 | Train Accuracy: 0.9168\nValidation Loss: 0.4065 | Validation Accuracy: 0.8625\nTime: 149.40 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 4\nTrain Loss: 0.1893 | Train Accuracy: 0.9360\nValidation Loss: 0.3592 | Validation Accuracy: 0.8619\nTime: 149.44 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 16\nEpoch 5\nTrain Loss: 0.1607 | Train Accuracy: 0.9442\nValidation Loss: 0.4030 | Validation Accuracy: 0.8654\nTime: 149.59 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 1\nTrain Loss: 0.1904 | Train Accuracy: 0.9378\nValidation Loss: 0.3994 | Validation Accuracy: 0.8672\nTime: 140.63 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 2\nTrain Loss: 0.1507 | Train Accuracy: 0.9527\nValidation Loss: 0.4443 | Validation Accuracy: 0.8661\nTime: 140.70 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 3\nTrain Loss: 0.1403 | Train Accuracy: 0.9556\nValidation Loss: 0.4271 | Validation Accuracy: 0.8632\nTime: 141.08 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 4\nTrain Loss: 0.1276 | Train Accuracy: 0.9611\nValidation Loss: 0.4440 | Validation Accuracy: 0.8690\nTime: 140.59 seconds\n--------------------------------------------------\nOptimizer: rmsprop | Learning Rate: 2e-05 | Batch Size: 32\nEpoch 5\nTrain Loss: 0.1196 | Train Accuracy: 0.9633\nValidation Loss: 0.4388 | Validation Accuracy: 0.8626\nTime: 141.14 seconds\n--------------------------------------------------\nBest Validation Accuracy: 0.8690\nBest Parameters: Optimizer = rmsprop, Learning Rate = 2e-05, Batch Size = 32\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score,\n    f1_score, roc_auc_score, confusion_matrix,\n    classification_report, roc_curve\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\ndef evaluate_binary_model(model, dataloader, device):\n    model.eval()\n    all_true_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids = b_input_ids.to(device)\n            b_input_mask = b_input_mask.to(device)\n            b_labels = b_labels.to(device)\n\n            outputs = model(b_input_ids, b_input_mask)\n            probs = torch.softmax(outputs, dim=1)\n\n            all_true_labels.append(b_labels.cpu().numpy())\n            all_probs.append(probs.cpu().numpy())\n\n    # Convert to numpy\n    all_true_labels = np.concatenate(all_true_labels)\n    all_probs = np.concatenate(all_probs)\n    all_preds = np.argmax(all_probs, axis=1)\n\n    # ================= Metrics =================\n    tn, fp, fn, tp = confusion_matrix(all_true_labels, all_preds).ravel()\n\n    accuracy = accuracy_score(all_true_labels, all_preds)\n    precision = precision_score(all_true_labels, all_preds)\n    recall = recall_score(all_true_labels, all_preds)          # Sensitivity\n    specificity = tn / (tn + fp)\n    f1 = f1_score(all_true_labels, all_preds)\n    auc = roc_auc_score(all_true_labels, all_probs[:, 1])\n\n    # ================= Print metrics =================\n    print(\"===== Evaluation Metrics =====\")\n    print(f\"Accuracy     : {accuracy:.4f}\")\n    print(f\"Precision    : {precision:.4f}\")\n    print(f\"Recall       : {recall:.4f}\")\n    print(f\"Sensitivity : {recall:.4f}\")\n    print(f\"Specificity : {specificity:.4f}\")\n    print(f\"F1-score    : {f1:.4f}\")\n    print(f\"AUC         : {auc:.4f}\")\n\n    # ================= Classification Report =================\n    print(\"\\n===== Classification Report =====\")\n    print(classification_report(all_true_labels, all_preds, digits=4))\n\n    # ================= ROC Curve (Both Classes) =================\n    fpr_0, tpr_0, _ = roc_curve(all_true_labels, all_probs[:, 0], pos_label=0)\n    fpr_1, tpr_1, _ = roc_curve(all_true_labels, all_probs[:, 1], pos_label=1)\n\n    auc_0 = roc_auc_score(1 - all_true_labels, all_probs[:, 0])\n    auc_1 = roc_auc_score(all_true_labels, all_probs[:, 1])\n\n    plt.figure(figsize=(7, 6))\n    plt.plot(fpr_0, tpr_0, label=f\"Class 0 ROC (AUC = {auc_0:.2f})\", lw=2)\n    plt.plot(fpr_1, tpr_1, label=f\"Class 1 ROC (AUC = {auc_1:.2f})\", lw=2)\n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curve for Binary Classification\")\n    plt.legend(loc=\"lower right\")\n    plt.grid()\n    plt.tight_layout()\n    plt.show()\n\n    return accuracy, precision, recall, specificity, f1, auc\n\n\naccuracy, precision, recall, specificity, f1, auc = evaluate_binary_model(\n    model, val_dataloader, device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:27:22.253249Z","iopub.execute_input":"2026-02-14T06:27:22.253465Z","iopub.status.idle":"2026-02-14T06:27:36.287311Z","shell.execute_reply.started":"2026-02-14T06:27:22.253445Z","shell.execute_reply":"2026-02-14T06:27:36.286717Z"}},"outputs":[{"name":"stdout","text":"===== Evaluation Metrics =====\nAccuracy     : 0.8629\nPrecision    : 0.7568\nRecall       : 0.7536\nSensitivity : 0.7536\nSpecificity : 0.9055\nF1-score    : 0.7552\nAUC         : 0.8953\n\n===== Classification Report =====\n              precision    recall  f1-score   support\n\n           0     0.9040    0.9055    0.9048      1238\n           1     0.7568    0.7536    0.7552       483\n\n    accuracy                         0.8629      1721\n   macro avg     0.8304    0.8296    0.8300      1721\nweighted avg     0.8627    0.8629    0.8628      1721\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 700x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import re, time, random, warnings\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pennylane as qml\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, accuracy_score\n\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.optim import AdamW\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nwarnings.filterwarnings(\"ignore\")\nnltk.download(\"stopwords\")\nstop_words = set(stopwords.words(\"english\"))\n\n\n# Install if needed\n# !pip install datasets nltk\n\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import load_dataset\n\n# ---------------------------\n# Load MPQA Dataset\n# ---------------------------\nds = load_dataset(\"jxm/mpqa\")\n\n# Convert train split to pandas (MPQA already split)\ndf = ds[\"train\"].to_pandas()\n\nprint(df.head())\nprint(df.shape)\n\n# MPQA typically has columns: 'text' and 'label'\n# If column name is different, print df.columns to check.\n\n# Rename for compatibility with your previous code\ndf = df.rename(columns={\"text\": \"review\"})\n\n# ---------------------------\n# Basic Preprocessing\n# ---------------------------\n\ndef preprocess_text(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    text = re.sub(r'@\\w+', '', text)\n    text = re.sub(r'#\\w+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = text.lower()\n    return text\n\ndf['cleaned_text'] = df['sentence'].apply(preprocess_text)\n\n# ---------------------------\n# Remove Stopwords\n# ---------------------------\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef clean_statement(statement):\n    statement = statement.lower()\n    statement = re.sub(r'[^\\w\\s]', '', statement)\n    statement = re.sub(r'\\d+', '', statement)\n    words = statement.split()\n    words = [word for word in words if word not in stop_words]\n    cleaned_statement = ' '.join(words)\n    return cleaned_statement\n\ndf['cleaned_text'] = df['cleaned_text'].apply(clean_statement)\n\nprint(df.head())\n\n# ---------------------------\n# Label Encoding\n# ---------------------------\n# MPQA labels are usually already numeric (0,1)\n# But we keep this step for safety\n\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['label'])\n\nprint(df['label'].value_counts())\n\n# ---------------------------\n# Balance Dataset\n# ---------------------------\n\nmajority_class = df['label'].value_counts().idxmax()\nminority_class = df['label'].value_counts().idxmin()\n\ndf_majority = df[df['label'] == majority_class]\ndf_minority = df[df['label'] == minority_class]\n\n# Downsample majority\ndf_majority_downsampled = df_majority.sample(len(df_minority), random_state=42)\n\n# Combine\ndf_balanced = pd.concat([df_majority_downsampled, df_minority])\n\n# Shuffle\ndf_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"Balanced Class Distribution:\")\nprint(df_balanced['label'].value_counts())\n\ndf_balanced.head()\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n\ntokens = tokenizer(\n    df_balanced[\"cleaned_text\"].tolist(),\n    padding=\"max_length\",\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)\n\ninput_ids = tokens[\"input_ids\"]\nattention_mask = tokens[\"attention_mask\"]\nlabels = torch.tensor(df_balanced[\"label\"].values)\nX_train, X_val, y_train, y_val, m_train, m_val = train_test_split(\n    input_ids, labels, attention_mask, test_size=0.2, random_state=42\n)\n\ntrain_data = TensorDataset(X_train, m_train, y_train)\nval_data = TensorDataset(X_val, m_val, y_val)\n\ntrain_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=16)\nval_loader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=16)\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc1(weights, x):\n    qml.Hadamard(0)\n    qml.Hadamard(1)\n    qml.RX(x[0], 0)\n    qml.RY(x[1], 1)\n    qml.CNOT([0,1])\n    qml.RZ(weights[0], 0)\n    qml.RZ(weights[1], 1)\n    return qml.expval(qml.PauliZ(0))\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc2(weights, x):\n    qml.Hadamard(0)\n    qml.RY(x[0], 0)\n    qml.RX(x[1], 1)\n    qml.CNOT([1,0])\n    qml.RX(weights[0], 0)\n    qml.RY(weights[1], 1)\n    return qml.expval(qml.PauliZ(0))\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc3(weights, x):\n    for i in range(2):\n        qml.Hadamard(i)\n        qml.RX(x[i], i)\n        qml.RY(weights[i], i)\n        qml.RZ(weights[i], i)\n    qml.CNOT([0,1])\n    return qml.expval(qml.PauliZ(1))\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc4(weights, x):\n    qml.Hadamard(0)\n    qml.CNOT([0,1])\n    qml.RX(x[0], 0)\n    qml.RY(x[1], 1)\n    qml.CNOT([1,0])\n    qml.RZ(weights[0], 0)\n    qml.RZ(weights[1], 1)\n    return qml.expval(qml.PauliZ(0))\n\nQUANTUM_CIRCUITS = {\n    \"QC1\": qc1,\n    \"QC2\": qc2,\n    \"QC3\": qc3,\n    \"QC4\": qc4\n}\nclass QBiLSTM(nn.Module):\n    def __init__(self, quantum_circuit):\n        super().__init__()\n        self.qc = quantum_circuit\n        self.q_weights = nn.Parameter(torch.randn(2))\n\n        self.encoder = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n\n        self.bilstm = nn.LSTM(\n            input_size=768,\n            hidden_size=128,\n            bidirectional=True,\n            batch_first=True\n        )\n\n        self.fc = nn.Linear(256, 2)\n\n    def quantum_layer(self, x):\n        q_outs = []\n        for v in x:\n            q_outs.append(self.qc(self.q_weights, v[:2]))\n        return torch.stack(q_outs)\n\n    def forward(self, input_ids, attention_mask):\n        enc = self.encoder(input_ids, attention_mask).last_hidden_state\n        lstm_out, _ = self.bilstm(enc)\n        h = lstm_out[:, -1, :]\n\n        _ = self.quantum_layer(h)  # quantum interaction\n\n        return self.fc(h)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresults = {}\n\nfor name, qc in QUANTUM_CIRCUITS.items():\n    print(f\"\\n====== Training {name} ======\")\n\n    model = QBiLSTM(qc).to(device)\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(10):\n        print(f\"\\n--- Epoch {epoch+1}/10 ---\")\n        model.train()\n        total_loss = 0\n\n        for step, batch in enumerate(train_loader, 1):\n            ids, mask, y = [b.to(device) for b in batch]\n\n            optimizer.zero_grad()\n            out = model(ids, mask)\n            loss = loss_fn(out, y)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            if step % 50 == 0:  # print every 50 steps\n                print(f\"Step {step}: Loss = {loss.item():.4f}\")\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1} finished. Avg Loss = {avg_loss:.4f}\")\n\n    # Evaluation\n    model.eval()\n    preds, gold = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            ids, mask, y = [b.to(device) for b in batch]\n            logits = model(ids, mask)\n            preds.extend(torch.argmax(logits, 1).cpu().numpy())\n            gold.extend(y.cpu().numpy())\n\n    acc = accuracy_score(gold, preds)\n    results[name] = acc\n\n    print(f\"{name} Accuracy: {acc:.4f}\")\n    print(classification_report(gold, preds))\n\nprint(\"\\n===== FINAL QUANTUM CIRCUIT COMPARISON =====\")\nfor k, v in results.items():\n    print(f\"{k}: {v:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:27:57.247976Z","iopub.execute_input":"2026-02-14T06:27:57.248740Z","iopub.status.idle":"2026-02-14T07:20:07.398297Z","shell.execute_reply.started":"2026-02-14T06:27:57.248711Z","shell.execute_reply":"2026-02-14T07:20:07.397424Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"                                  sentence  label\n0         would not find it at all strange      0\n1  that four shots would solve the problem      0\n2                                  in turn      0\n3                                  because      0\n4                                 regained      0\n(8603, 2)\n                                  sentence  label  \\\n0         would not find it at all strange      0   \n1  that four shots would solve the problem      0   \n2                                  in turn      0   \n3                                  because      0   \n4                                 regained      0   \n\n                     cleaned_text  \n0              would find strange  \n1  four shots would solve problem  \n2                            turn  \n3                                  \n4                        regained  \nlabel\n0    6292\n1    2311\nName: count, dtype: int64\nBalanced Class Distribution:\nlabel\n0    2311\n1    2311\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"\n====== Training QC1 ======\n\n--- Epoch 1/10 ---\nStep 50: Loss = 0.6693\nStep 100: Loss = 0.7035\nStep 150: Loss = 0.7086\nStep 200: Loss = 0.7026\nEpoch 1 finished. Avg Loss = 0.6988\n\n--- Epoch 2/10 ---\nStep 50: Loss = 0.6962\nStep 100: Loss = 0.6931\nStep 150: Loss = 0.6743\nStep 200: Loss = 0.6969\nEpoch 2 finished. Avg Loss = 0.6950\n\n--- Epoch 3/10 ---\nStep 50: Loss = 0.6875\nStep 100: Loss = 0.7088\nStep 150: Loss = 0.6886\nStep 200: Loss = 0.6909\nEpoch 3 finished. Avg Loss = 0.6939\n\n--- Epoch 4/10 ---\nStep 50: Loss = 0.6974\nStep 100: Loss = 0.6799\nStep 150: Loss = 0.6763\nStep 200: Loss = 0.6966\nEpoch 4 finished. Avg Loss = 0.6949\n\n--- Epoch 5/10 ---\nStep 50: Loss = 0.6957\nStep 100: Loss = 0.6932\nStep 150: Loss = 0.6835\nStep 200: Loss = 0.6897\nEpoch 5 finished. Avg Loss = 0.6938\n\n--- Epoch 6/10 ---\nStep 50: Loss = 0.6969\nStep 100: Loss = 0.6847\nStep 150: Loss = 0.6641\nStep 200: Loss = 0.6567\nEpoch 6 finished. Avg Loss = 0.6926\n\n--- Epoch 7/10 ---\nStep 50: Loss = 0.6882\nStep 100: Loss = 0.6736\nStep 150: Loss = 0.6807\nStep 200: Loss = 0.6485\nEpoch 7 finished. Avg Loss = 0.6743\n\n--- Epoch 8/10 ---\nStep 50: Loss = 0.2931\nStep 100: Loss = 0.4374\nStep 150: Loss = 0.3429\nStep 200: Loss = 0.2570\nEpoch 8 finished. Avg Loss = 0.4597\n\n--- Epoch 9/10 ---\nStep 50: Loss = 0.7592\nStep 100: Loss = 0.1047\nStep 150: Loss = 0.4242\nStep 200: Loss = 0.4594\nEpoch 9 finished. Avg Loss = 0.3664\n\n--- Epoch 10/10 ---\nStep 50: Loss = 0.3614\nStep 100: Loss = 0.2176\nStep 150: Loss = 0.3289\nStep 200: Loss = 0.1407\nEpoch 10 finished. Avg Loss = 0.3194\nQC1 Accuracy: 0.8530\n              precision    recall  f1-score   support\n\n           0       0.83      0.88      0.86       459\n           1       0.88      0.82      0.85       466\n\n    accuracy                           0.85       925\n   macro avg       0.85      0.85      0.85       925\nweighted avg       0.85      0.85      0.85       925\n\n\n====== Training QC2 ======\n\n--- Epoch 1/10 ---\nStep 50: Loss = 0.6974\nStep 100: Loss = 0.6944\nStep 150: Loss = 0.7020\nStep 200: Loss = 0.6891\nEpoch 1 finished. Avg Loss = 0.6956\n\n--- Epoch 2/10 ---\nStep 50: Loss = 0.6951\nStep 100: Loss = 0.6840\nStep 150: Loss = 0.6884\nStep 200: Loss = 0.6928\nEpoch 2 finished. Avg Loss = 0.6945\n\n--- Epoch 3/10 ---\nStep 50: Loss = 0.6985\nStep 100: Loss = 0.7045\nStep 150: Loss = 0.6970\nStep 200: Loss = 0.6901\nEpoch 3 finished. Avg Loss = 0.6940\n\n--- Epoch 4/10 ---\nStep 50: Loss = 0.6937\nStep 100: Loss = 0.6998\nStep 150: Loss = 0.6952\nStep 200: Loss = 0.7021\nEpoch 4 finished. Avg Loss = 0.6947\n\n--- Epoch 5/10 ---\nStep 50: Loss = 0.7017\nStep 100: Loss = 0.7037\nStep 150: Loss = 0.6943\nStep 200: Loss = 0.6863\nEpoch 5 finished. Avg Loss = 0.6939\n\n--- Epoch 6/10 ---\nStep 50: Loss = 0.7008\nStep 100: Loss = 0.6891\nStep 150: Loss = 0.6937\nStep 200: Loss = 0.7064\nEpoch 6 finished. Avg Loss = 0.6938\n\n--- Epoch 7/10 ---\nStep 50: Loss = 0.6846\nStep 100: Loss = 0.8085\nStep 150: Loss = 0.6823\nStep 200: Loss = 0.7044\nEpoch 7 finished. Avg Loss = 0.6933\n\n--- Epoch 8/10 ---\nStep 50: Loss = 0.6570\nStep 100: Loss = 0.6907\nStep 150: Loss = 0.5300\nStep 200: Loss = 0.6538\nEpoch 8 finished. Avg Loss = 0.6160\n\n--- Epoch 9/10 ---\nStep 50: Loss = 0.5717\nStep 100: Loss = 0.5943\nStep 150: Loss = 0.4641\nStep 200: Loss = 0.6430\nEpoch 9 finished. Avg Loss = 0.5809\n\n--- Epoch 10/10 ---\nStep 50: Loss = 0.2746\nStep 100: Loss = 0.4498\nStep 150: Loss = 0.4383\nStep 200: Loss = 0.3140\nEpoch 10 finished. Avg Loss = 0.4916\nQC2 Accuracy: 0.8335\n              precision    recall  f1-score   support\n\n           0       0.83      0.84      0.83       459\n           1       0.84      0.82      0.83       466\n\n    accuracy                           0.83       925\n   macro avg       0.83      0.83      0.83       925\nweighted avg       0.83      0.83      0.83       925\n\n\n====== Training QC3 ======\n\n--- Epoch 1/10 ---\nStep 50: Loss = 0.6912\nStep 100: Loss = 0.6923\nStep 150: Loss = 0.6944\nStep 200: Loss = 0.6851\nEpoch 1 finished. Avg Loss = 0.6947\n\n--- Epoch 2/10 ---\nStep 50: Loss = 0.6981\nStep 100: Loss = 0.6899\nStep 150: Loss = 0.6916\nStep 200: Loss = 0.6881\nEpoch 2 finished. Avg Loss = 0.6948\n\n--- Epoch 3/10 ---\nStep 50: Loss = 0.7014\nStep 100: Loss = 0.6962\nStep 150: Loss = 0.6909\nStep 200: Loss = 0.6810\nEpoch 3 finished. Avg Loss = 0.6929\n\n--- Epoch 4/10 ---\nStep 50: Loss = 0.6937\nStep 100: Loss = 0.6865\nStep 150: Loss = 0.6967\nStep 200: Loss = 0.6968\nEpoch 4 finished. Avg Loss = 0.6936\n\n--- Epoch 5/10 ---\nStep 50: Loss = 0.4933\nStep 100: Loss = 0.4503\nStep 150: Loss = 0.6232\nStep 200: Loss = 0.2049\nEpoch 5 finished. Avg Loss = 0.4712\n\n--- Epoch 6/10 ---\nStep 50: Loss = 0.1221\nStep 100: Loss = 0.1528\nStep 150: Loss = 0.1078\nStep 200: Loss = 0.2132\nEpoch 6 finished. Avg Loss = 0.3227\n\n--- Epoch 7/10 ---\nStep 50: Loss = 0.3171\nStep 100: Loss = 0.1239\nStep 150: Loss = 0.0781\nStep 200: Loss = 0.1541\nEpoch 7 finished. Avg Loss = 0.2733\n\n--- Epoch 8/10 ---\nStep 50: Loss = 0.3749\nStep 100: Loss = 0.3818\nStep 150: Loss = 0.0371\nStep 200: Loss = 0.0518\nEpoch 8 finished. Avg Loss = 0.2268\n\n--- Epoch 9/10 ---\nStep 50: Loss = 0.0507\nStep 100: Loss = 0.0538\nStep 150: Loss = 0.3621\nStep 200: Loss = 0.3007\nEpoch 9 finished. Avg Loss = 0.1978\n\n--- Epoch 10/10 ---\nStep 50: Loss = 0.2564\nStep 100: Loss = 0.1440\nStep 150: Loss = 0.4052\nStep 200: Loss = 0.0296\nEpoch 10 finished. Avg Loss = 0.1730\nQC3 Accuracy: 0.8714\n              precision    recall  f1-score   support\n\n           0       0.87      0.87      0.87       459\n           1       0.87      0.87      0.87       466\n\n    accuracy                           0.87       925\n   macro avg       0.87      0.87      0.87       925\nweighted avg       0.87      0.87      0.87       925\n\n\n====== Training QC4 ======\n\n--- Epoch 1/10 ---\nStep 50: Loss = 0.7079\nStep 100: Loss = 0.7087\nStep 150: Loss = 0.6956\nStep 200: Loss = 0.7144\nEpoch 1 finished. Avg Loss = 0.6964\n\n--- Epoch 2/10 ---\nStep 50: Loss = 0.6891\nStep 100: Loss = 0.6891\nStep 150: Loss = 0.6813\nStep 200: Loss = 0.7296\nEpoch 2 finished. Avg Loss = 0.6958\n\n--- Epoch 3/10 ---\nStep 50: Loss = 0.6513\nStep 100: Loss = 0.7042\nStep 150: Loss = 0.6960\nStep 200: Loss = 0.6852\nEpoch 3 finished. Avg Loss = 0.6942\n\n--- Epoch 4/10 ---\nStep 50: Loss = 0.6737\nStep 100: Loss = 0.6867\nStep 150: Loss = 0.6814\nStep 200: Loss = 0.6680\nEpoch 4 finished. Avg Loss = 0.6893\n\n--- Epoch 5/10 ---\nStep 50: Loss = 0.6435\nStep 100: Loss = 0.4934\nStep 150: Loss = 0.5081\nStep 200: Loss = 0.5249\nEpoch 5 finished. Avg Loss = 0.5691\n\n--- Epoch 6/10 ---\nStep 50: Loss = 0.6551\nStep 100: Loss = 0.4124\nStep 150: Loss = 0.6759\nStep 200: Loss = 0.2475\nEpoch 6 finished. Avg Loss = 0.3779\n\n--- Epoch 7/10 ---\nStep 50: Loss = 0.4682\nStep 100: Loss = 0.4446\nStep 150: Loss = 0.1870\nStep 200: Loss = 0.1514\nEpoch 7 finished. Avg Loss = 0.3509\n\n--- Epoch 8/10 ---\nStep 50: Loss = 0.0751\nStep 100: Loss = 0.4456\nStep 150: Loss = 0.2499\nStep 200: Loss = 0.2279\nEpoch 8 finished. Avg Loss = 0.2938\n\n--- Epoch 9/10 ---\nStep 50: Loss = 0.3584\nStep 100: Loss = 0.4152\nStep 150: Loss = 0.2902\nStep 200: Loss = 0.1382\nEpoch 9 finished. Avg Loss = 0.2680\n\n--- Epoch 10/10 ---\nStep 50: Loss = 0.4507\nStep 100: Loss = 0.1044\nStep 150: Loss = 0.2745\nStep 200: Loss = 0.1441\nEpoch 10 finished. Avg Loss = 0.2562\nQC4 Accuracy: 0.8649\n              precision    recall  f1-score   support\n\n           0       0.86      0.86      0.86       459\n           1       0.87      0.86      0.87       466\n\n    accuracy                           0.86       925\n   macro avg       0.86      0.86      0.86       925\nweighted avg       0.86      0.86      0.86       925\n\n\n===== FINAL QUANTUM CIRCUIT COMPARISON =====\nQC1: 0.8530\nQC2: 0.8335\nQC3: 0.8714\nQC4: 0.8649\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install pennylane\n# =========================\n# 1. Imports\n# =========================\nimport re, time, random, warnings\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pennylane as qml\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split, ParameterSampler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nwarnings.filterwarnings(\"ignore\")\nnltk.download(\"stopwords\")\nstop_words = set(stopwords.words(\"english\"))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =========================\n# 2. Load & Clean Dataset\n# =========================\nds = load_dataset(\"jxm/mpqa\")\n\n# Convert train split to pandas (MPQA already split)\ndf = ds[\"train\"].to_pandas()\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    text = re.sub(r\"\\d+\", \"\", text)\n    words = [w for w in text.split() if w not in stop_words]\n    return \" \".join(words)\n\ndf[\"cleaned_text\"] = df[\"sentence\"].astype(str).apply(clean_text)\n\nle = LabelEncoder()\ndf[\"label\"] = le.fit_transform(df[\"label\"])\n\n# Balance classes\nmin_class = df[\"label\"].value_counts().min()\ndf = (\n    df.groupby(\"label\", group_keys=False)\n      .apply(lambda x: x.sample(min_class, random_state=42))\n      .sample(frac=1, random_state=42)\n)\n\n# =========================\n# 3. Tokenization\n# =========================\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n\ntokens = tokenizer(\n    df[\"cleaned_text\"].tolist(),\n    padding=\"max_length\",\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)\n\ninput_ids = tokens[\"input_ids\"]\nattention_mask = tokens[\"attention_mask\"]\nlabels = torch.tensor(df[\"label\"].values)\n\nX_train, X_val, y_train, y_val, m_train, m_val = train_test_split(\n    input_ids, labels, attention_mask, test_size=0.2, random_state=42\n)\n\ntrain_data = TensorDataset(X_train, m_train, y_train)\nval_data   = TensorDataset(X_val, m_val, y_val)\n\n# =========================\n# 4. Quantum Circuits\n# =========================\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc1(weights, x):\n    qml.Hadamard(0)\n    qml.Hadamard(1)\n    qml.RX(x[0], 0)\n    qml.RY(x[1], 1)\n    qml.CNOT([0, 1])\n    qml.RZ(weights[0], 0)\n    qml.RZ(weights[1], 1)\n    return qml.expval(qml.PauliZ(0))\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc2(weights, x):\n    qml.Hadamard(0)\n    qml.RY(x[0], 0)\n    qml.RX(x[1], 1)\n    qml.CNOT([1, 0])\n    qml.RX(weights[0], 0)\n    qml.RY(weights[1], 1)\n    return qml.expval(qml.PauliZ(0))\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc3(weights, x):\n    for i in range(2):\n        qml.Hadamard(i)\n        qml.RX(x[i], i)\n        qml.RY(weights[i], i)\n        qml.RZ(weights[i], i)\n    qml.CNOT([0, 1])\n    return qml.expval(qml.PauliZ(1))\n\n@qml.qnode(dev, interface=\"torch\")\ndef qc4(weights, x):\n    qml.Hadamard(0)\n    qml.CNOT([0, 1])\n    qml.RX(x[0], 0)\n    qml.RY(x[1], 1)\n    qml.CNOT([1, 0])\n    qml.RZ(weights[0], 0)\n    qml.RZ(weights[1], 1)\n    return qml.expval(qml.PauliZ(0))\n\nQUANTUM_CIRCUITS = {\n    \"QC1\": qc1,\n    \"QC2\": qc2,\n    \"QC3\": qc3,\n    \"QC4\": qc4\n}\n\n# =========================\n# 5. QBiLSTM Model\n# =========================\nclass QBiLSTM(nn.Module):\n    def __init__(self, quantum_circuit):\n        super().__init__()\n        self.qc = quantum_circuit\n        self.q_weights = nn.Parameter(torch.randn(2))\n\n        self.encoder = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n\n        self.bilstm = nn.LSTM(\n            input_size=768,\n            hidden_size=128,\n            bidirectional=True,\n            batch_first=True\n        )\n\n        self.fc = nn.Linear(257, 2)  # 256 + quantum feature\n\n    def quantum_layer(self, x):\n        q_out = [self.qc(self.q_weights, v[:2]) for v in x]\n        q_out = torch.stack(q_out).unsqueeze(1)\n        return q_out.float()\n\n    def forward(self, input_ids, attention_mask):\n        enc = self.encoder(input_ids, attention_mask).last_hidden_state\n        lstm_out, _ = self.bilstm(enc)\n        h = lstm_out[:, -1, :]\n\n        q_feat = self.quantum_layer(h)\n        h = torch.cat([h, q_feat], dim=1)\n\n        return self.fc(h)\n\n# =========================\n# 6. Training Function\n# =========================\ndef train_and_validate(model, params):\n    train_loader = DataLoader(\n        train_data, sampler=RandomSampler(train_data),\n        batch_size=params[\"batch_size\"]\n    )\n    val_loader = DataLoader(\n        val_data, sampler=SequentialSampler(val_data),\n        batch_size=params[\"batch_size\"]\n    )\n\n    optimizer = AdamW(model.parameters(), lr=params[\"learning_rate\"])\n    loss_fn = nn.CrossEntropyLoss()\n\n    total_steps = len(train_loader) * params[\"epochs\"]\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, 0, total_steps\n    )\n\n    for _ in range(params[\"epochs\"]):\n        model.train()\n        for ids, mask, y in train_loader:\n            ids, mask, y = ids.to(device), mask.to(device), y.to(device)\n\n            optimizer.zero_grad()\n            loss = loss_fn(model(ids, mask), y)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n    model.eval()\n    preds, gold = [], []\n    with torch.no_grad():\n        for ids, mask, y in val_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(ids, mask)\n            preds.extend(torch.argmax(logits, 1).cpu().numpy())\n            gold.extend(y.numpy())\n\n    return accuracy_score(gold, preds)\n\n# =========================\n# 7. Hyperparameter Search\n# =========================\nparam_grid = {\n    \"learning_rate\": [1e-5, 2e-5, 3e-5],\n    \"batch_size\": [16, 32],\n    \"epochs\": [3, 5]\n}\n\nparam_list = list(ParameterSampler(param_grid, n_iter=5, random_state=42))\n\nfinal_results = {}\n\nfor name, qc in QUANTUM_CIRCUITS.items():\n    print(f\"\\n===== {name} =====\")\n\n    best_acc = 0\n    best_params = None\n\n    for params in param_list:\n        model = QBiLSTM(qc).to(device)\n        acc = train_and_validate(model, params)\n\n        print(f\"{params} â†’ Acc: {acc:.4f}\")\n\n        if acc > best_acc:\n            best_acc = acc\n            best_params = params\n\n    final_results[name] = (best_acc, best_params)\n\n# =========================\n# 8. Final Results\n# =========================\nprint(\"\\n===== FINAL COMPARISON =====\")\nfor k, (acc, params) in final_results.items():\n    print(f\"{k}: Accuracy = {acc:.4f}, Params = {params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T07:46:44.945939Z","iopub.execute_input":"2026-02-14T07:46:44.946243Z","iopub.status.idle":"2026-02-14T09:46:01.107680Z","shell.execute_reply.started":"2026-02-14T07:46:44.946218Z","shell.execute_reply":"2026-02-14T09:46:01.106958Z"}},"outputs":[{"name":"stdout","text":"Collecting pennylane\n  Downloading pennylane-0.44.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.15.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\nCollecting rustworkx>=0.14.0 (from pennylane)\n  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\nCollecting appdirs (from pennylane)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nCollecting autoray==0.8.2 (from pennylane)\n  Downloading autoray-0.8.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\nCollecting pennylane-lightning>=0.44 (from pennylane)\n  Downloading pennylane_lightning-0.44.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.5)\nRequirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (26.0rc2)\nCollecting diastatic-malt (from pennylane)\n  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\nCollecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.44->pennylane)\n  Downloading scipy_openblas32-0.3.31.22.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\nRequirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2026.1.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\nRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\nDownloading pennylane-0.44.0-py3-none-any.whl (5.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading autoray-0.8.2-py3-none-any.whl (935 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m935.6/935.6 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pennylane_lightning-0.44.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scipy_openblas32-0.3.31.22.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\nSuccessfully installed appdirs-1.4.4 autoray-0.8.2 diastatic-malt-2.15.2 pennylane-0.44.0 pennylane-lightning-0.44.0 rustworkx-0.17.1 scipy-openblas32-0.3.31.22.1\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be6ea329c1740328d89059e5b5f0a82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f5c43f656f42cf9a26b0004aff95e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a83d35d98c4d42b1ee7d9b592e3efd"}},"metadata":{}},{"name":"stdout","text":"\n===== QC1 =====\n","output_type":"stream"},{"name":"stderr","text":"2026-02-14 07:47:16.046636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771055236.232994      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771055236.281383      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771055236.694404      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771055236.694442      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771055236.694445      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771055236.694447      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22d2cf02803c46709f9ba0dbd1419f6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6619ab5c2f4f2793cd0349473a91e7"}},"metadata":{}},{"name":"stdout","text":"{'learning_rate': 2e-05, 'epochs': 5, 'batch_size': 32} â†’ Acc: 0.7838\n{'learning_rate': 1e-05, 'epochs': 5, 'batch_size': 32} â†’ Acc: 0.6865\n{'learning_rate': 1e-05, 'epochs': 3, 'batch_size': 16} â†’ Acc: 0.4962\n{'learning_rate': 3e-05, 'epochs': 3, 'batch_size': 32} â†’ Acc: 0.7535\n{'learning_rate': 3e-05, 'epochs': 5, 'batch_size': 16} â†’ Acc: 0.8551\n\n===== QC2 =====\n{'learning_rate': 2e-05, 'epochs': 5, 'batch_size': 32} â†’ Acc: 0.6238\n{'learning_rate': 1e-05, 'epochs': 5, 'batch_size': 32} â†’ Acc: 0.5049\n{'learning_rate': 1e-05, 'epochs': 3, 'batch_size': 16} â†’ Acc: 0.7416\n{'learning_rate': 3e-05, 'epochs': 3, 'batch_size': 32} â†’ Acc: 0.5038\n{'learning_rate': 3e-05, 'epochs': 5, 'batch_size': 16} â†’ Acc: 0.8465\n\n===== QC3 =====\n{'learning_rate': 2e-05, 'epochs': 5, 'batch_size': 32} â†’ Acc: 0.7557\n{'learning_rate': 1e-05, 'epochs': 5, 'batch_size': 32} â†’ Acc: 0.5038\n{'learning_rate': 1e-05, 'epochs': 3, 'batch_size': 16} â†’ Acc: 0.5622\n{'learning_rate': 3e-05, 'epochs': 3, 'batch_size': 32} â†’ Acc: 0.7243\n{'learning_rate': 3e-05, 'epochs': 5, 'batch_size': 16} â†’ Acc: 0.8627\n\n===== QC4 =====\n{'learning_rate': 2e-05, 'epochs': 5, 'batch_size': 32} â†’ Acc: 0.7157\n{'learning_rate': 1e-05, 'epochs': 5, 'batch_size': 32} â†’ Acc: 0.7503\n{'learning_rate': 1e-05, 'epochs': 3, 'batch_size': 16} â†’ Acc: 0.5232\n{'learning_rate': 3e-05, 'epochs': 3, 'batch_size': 32} â†’ Acc: 0.8551\n{'learning_rate': 3e-05, 'epochs': 5, 'batch_size': 16} â†’ Acc: 0.5049\n\n===== FINAL COMPARISON =====\nQC1: Accuracy = 0.8551, Params = {'learning_rate': 3e-05, 'epochs': 5, 'batch_size': 16}\nQC2: Accuracy = 0.8465, Params = {'learning_rate': 3e-05, 'epochs': 5, 'batch_size': 16}\nQC3: Accuracy = 0.8627, Params = {'learning_rate': 3e-05, 'epochs': 5, 'batch_size': 16}\nQC4: Accuracy = 0.8551, Params = {'learning_rate': 3e-05, 'epochs': 3, 'batch_size': 32}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"epochs = 5\nlearning_rates = [2e-5, 3e-5, 5e-5]\nbatch_sizes = [16, 32]\noptimizers = ['adamw', 'adam', 'rmsprop', 'sgd']\nnum_samples = 5\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ndef get_optimizer(optimizer_name, model_parameters, lr):\n    if optimizer_name == 'adamw':\n        return optim.AdamW(model_parameters, lr=lr)\n    elif optimizer_name == 'adam':\n        return optim.Adam(model_parameters, lr=lr)\n    elif optimizer_name == 'rmsprop':\n        return optim.RMSprop(model_parameters, lr=lr)\n    elif optimizer_name == 'sgd':\n        return optim.SGD(model_parameters, lr=lr)\nfinal_results = {}\n\nfor qc_name, qc in QUANTUM_CIRCUITS.items():\n    print(f\"\\n==============================\")\n    print(f\" Quantum Circuit: {qc_name}\")\n    print(f\"==============================\")\n\n    best_accuracy = 0\n    best_params = None\n\n    # Random hyperparameter sampling\n    random_hyperparams = [\n        {\n            \"optimizer\": random.choice(optimizers),\n            \"learning_rate\": random.choice(learning_rates),\n            \"batch_size\": random.choice(batch_sizes),\n        }\n        for _ in range(num_samples)\n    ]\n\n    for params in random_hyperparams:\n        optimizer_name = params[\"optimizer\"]\n        lr = params[\"learning_rate\"]\n        batch_size = params[\"batch_size\"]\n\n        print(f\"\\n Testing {params}\")\n\n        # Fresh model for each trial (VERY IMPORTANT)\n        model = QBiLSTM(qc).to(device)\n\n        train_dataloader = DataLoader(\n            train_data,\n            sampler=RandomSampler(train_data),\n            batch_size=batch_size\n        )\n\n        val_dataloader = DataLoader(\n            val_data,\n            sampler=SequentialSampler(val_data),\n            batch_size=batch_size\n        )\n\n        optimizer = get_optimizer(optimizer_name, model.parameters(), lr)\n        loss_fn = nn.CrossEntropyLoss()\n\n        total_steps = len(train_dataloader) * epochs\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=0,\n            num_training_steps=total_steps\n        )\n\n        # ===== Training =====\n        for epoch in range(epochs):\n            model.train()\n            total_train_loss = 0\n            total_train_accuracy = 0\n\n            for batch in train_dataloader:\n                b_input_ids, b_input_mask, b_labels = batch\n                b_input_ids = b_input_ids.to(device)\n                b_input_mask = b_input_mask.to(device)\n                b_labels = b_labels.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(b_input_ids, b_input_mask)\n                loss = loss_fn(outputs, b_labels)\n\n                total_train_loss += loss.item()\n                logits = outputs.detach().cpu().numpy()\n                total_train_accuracy += flat_accuracy(logits, b_labels.cpu().numpy())\n\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n\n            avg_train_loss = total_train_loss / len(train_dataloader)\n            avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n\n            # ===== Validation =====\n            model.eval()\n            total_val_accuracy = 0\n            total_val_loss = 0\n\n            with torch.no_grad():\n                for batch in val_dataloader:\n                    b_input_ids, b_input_mask, b_labels = batch\n                    b_input_ids = b_input_ids.to(device)\n                    b_input_mask = b_input_mask.to(device)\n                    b_labels = b_labels.to(device)\n\n                    outputs = model(b_input_ids, b_input_mask)\n                    loss = loss_fn(outputs, b_labels)\n\n                    total_val_loss += loss.item()\n                    logits = outputs.detach().cpu().numpy()\n                    total_val_accuracy += flat_accuracy(logits, b_labels.cpu().numpy())\n\n            avg_val_accuracy = total_val_accuracy / len(val_dataloader)\n\n            print(\n                f\"Epoch {epoch+1} | \"\n                f\"Train Acc: {avg_train_accuracy:.4f} | \"\n                f\"Val Acc: {avg_val_accuracy:.4f}\"\n            )\n\n        # ===== Track best =====\n        if avg_val_accuracy > best_accuracy:\n            best_accuracy = avg_val_accuracy\n            best_params = {\n                \"optimizer\": optimizer_name,\n                \"learning_rate\": lr,\n                \"batch_size\": batch_size\n            }\n\n    final_results[qc_name] = {\n        \"best_accuracy\": best_accuracy,\n        \"best_params\": best_params\n    }\n\n    print(f\"\\n BEST for {qc_name}: {best_accuracy:.4f}\")\n    print(f\" PARAMS: {best_params}\")\n\nprint(\"\\n===== FINAL QUANTUM CIRCUIT COMPARISON =====\")\nfor qc, res in final_results.items():\n    print(\n        f\"{qc} | Accuracy: {res['best_accuracy']:.4f} | \"\n        f\"Params: {res['best_params']}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:52:00.374365Z","iopub.execute_input":"2026-02-14T09:52:00.375119Z","iopub.status.idle":"2026-02-14T09:59:27.146315Z","shell.execute_reply.started":"2026-02-14T09:52:00.375091Z","shell.execute_reply":"2026-02-14T09:59:27.145255Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Quantum Circuit: QC1\n==============================\n\n Testing {'optimizer': 'rmsprop', 'learning_rate': 2e-05, 'batch_size': 16}\nEpoch 1 | Train Acc: 0.5005 | Val Acc: 0.5044\nEpoch 2 | Train Acc: 0.5075 | Val Acc: 0.4956\nEpoch 3 | Train Acc: 0.5800 | Val Acc: 0.5098\nEpoch 4 | Train Acc: 0.8055 | Val Acc: 0.8586\nEpoch 5 | Train Acc: 0.8866 | Val Acc: 0.8597\n\n Testing {'optimizer': 'adam', 'learning_rate': 2e-05, 'batch_size': 32}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1729885088.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/952852575.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    782\u001b[0m         )\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rel_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             output_states, attn_weights = layer_module(\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0mnext_kv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[0;32m--> 436\u001b[0;31m         attention_output, att_matrix = self.attention(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[0;32m--> 369\u001b[0;31m         self_output, att_matrix = self.self(\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             rel_att = self.disentangled_attention_bias(\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mdisentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    301\u001b[0m             pos_query_layer = self.transpose_for_scores(\n\u001b[1;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             ).repeat(query_layer.size(0) // self.num_attention_heads, 1, 1)\n\u001b[0m\u001b[1;32m    304\u001b[0m             pos_key_layer = self.transpose_for_scores(self.key_proj(rel_embeddings), self.num_attention_heads).repeat(\n\u001b[1;32m    305\u001b[0m                 \u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 21.12 MiB is free. Process 2818 has 15.87 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 116.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 21.12 MiB is free. Process 2818 has 15.87 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 116.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score,\n    roc_curve,\n    confusion_matrix\n)\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n# ----- Evaluation function -----\ndef evaluate_model(model, dataloader, num_classes):\n    model.eval()\n\n    all_logits = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids = b_input_ids.to(device)\n            b_input_mask = b_input_mask.to(device)\n\n            outputs = model(b_input_ids, b_input_mask)\n\n            all_logits.append(outputs.cpu())\n            all_labels.append(b_labels)\n\n    logits = torch.cat(all_logits).numpy()\n    labels = torch.cat(all_labels).numpy()\n\n    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n    preds = np.argmax(probs, axis=1)\n\n    # ---- Metrics ----\n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds, average=\"macro\")\n    sensitivity = recall_score(labels, preds, average=\"macro\")  # Recall\n    f1 = f1_score(labels, preds, average=\"macro\")\n\n    # ---- Specificity ----\n    cm = confusion_matrix(labels, preds)\n    specificity_per_class = []\n    for i in range(num_classes):\n        tn = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n        fp = cm[:, i].sum() - cm[i, i]\n        specificity_per_class.append(tn / (tn + fp + 1e-9))\n    specificity = np.mean(specificity_per_class)\n\n    # ---- AUC ----\n    if num_classes == 2:\n        auc = roc_auc_score(labels, probs[:, 1])  # binary case\n    else:\n        labels_bin = label_binarize(labels, classes=list(range(num_classes)))\n        auc = roc_auc_score(labels_bin, probs, average=\"macro\", multi_class=\"ovr\")\n\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"sensitivity\": sensitivity,\n        \"specificity\": specificity,\n        \"f1\": f1,\n        \"auc\": auc,\n        \"labels\": labels,\n        \"probs\": probs\n    }\n\n# ----- Number of classes -----\nnum_classes = len(torch.unique(train_data.tensors[2]))  # or 2 for binary\n\nbest_models_results = {}\n\n# ----- Evaluate each best model -----\nfor qc_name, res in final_results.items():\n    print(f\"\\nEvaluating BEST model for {qc_name}\")\n\n    params = res[\"best_params\"]\n\n    # Fresh model\n    model = QBiLSTM(QUANTUM_CIRCUITS[qc_name]).to(device)\n\n    train_loader = DataLoader(\n        train_data,\n        sampler=RandomSampler(train_data),\n        batch_size=params[\"batch_size\"]\n    )\n\n    val_loader = DataLoader(\n        val_data,\n        sampler=SequentialSampler(val_data),\n        batch_size=params[\"batch_size\"]\n    )\n\n    optimizer = get_optimizer(params[\"optimizer\"], model.parameters(), params[\"learning_rate\"])\n    loss_fn = nn.CrossEntropyLoss()\n\n    # ---- Retrain ----\n    for epoch in range(epochs):\n        model.train()\n        for batch in train_loader:\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(b_input_ids, b_input_mask)\n            loss = loss_fn(outputs, b_labels)\n            loss.backward()\n            optimizer.step()\n\n    # ---- Final Evaluation ----\n    metrics = evaluate_model(model, val_loader, num_classes)\n    best_models_results[qc_name] = metrics\n\n# ----- Print Summary Table -----\nprint(\"\\n===== FINAL PERFORMANCE COMPARISON =====\")\nprint(\"Model | Accuracy | Precision | Sensitivity | Specificity | F1 | AUC\")\nprint(\"-\"*75)\nfor qc, m in best_models_results.items():\n    print(f\"{qc} | {m['accuracy']:.4f} | {m['precision']:.4f} | {m['sensitivity']:.4f} | {m['specificity']:.4f} | {m['f1']:.4f} | {m['auc']:.4f}\")\n\n# ----- Plot ROC Curves -----\nfor qc, m in best_models_results.items():\n    labels = m[\"labels\"]\n    probs = m[\"probs\"]\n\n    plt.figure(figsize=(7,6))\n    if num_classes == 2:\n        fpr, tpr, _ = roc_curve(labels, probs[:,1])\n        auc_score = roc_auc_score(labels, probs[:,1])\n        plt.plot(fpr, tpr, label=f\"AUC={auc_score:.3f}\")\n    else:\n        labels_bin = label_binarize(labels, classes=list(range(num_classes)))\n        for i in range(num_classes):\n            fpr, tpr, _ = roc_curve(labels_bin[:, i], probs[:, i])\n            auc_i = roc_auc_score(labels_bin[:, i], probs[:, i])\n            plt.plot(fpr, tpr, label=f\"Class {i} (AUC={auc_i:.3f})\")\n\n    plt.plot([0,1], [0,1], linestyle=\"--\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(f\"ROC Curve â€“ {qc}\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-14T07:49:53.065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}